{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2 - Lab 2: Documenting Key Decisions with ADRs\n",
    "\n",
    "**Objective:** Use an LLM as a research assistant to compare technical options and synthesize the findings into a formal, version-controlled Architectural Decision Record (ADR).\n",
    "\n",
    "**Estimated Time:** 60 minutes\n",
    "\n",
    "**Introduction:**\n",
    "Great architectural decisions are based on research and trade-offs. A critical practice for healthy, long-lived projects is documenting *why* these decisions were made. In this lab, you will use an LLM to research a key technical choice for our application and then generate a formal ADR to record that decision for the future.\n",
    "\n",
    "For definitions of key terms used in this lab, please refer to the [GLOSSARY.md](../../GLOSSARY.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "We'll start by ensuring our environment is ready and adding the standard pathing solution to reliably import our `utils.py` helper.\n",
    "\n",
    "**Model Selection:**\n",
    "For research and synthesis tasks, models with large context windows and strong reasoning abilities are ideal. `gpt-4.1`, `gemini-2.5-pro`, or `meta-llama/Llama-3.3-70B-Instruct` would be excellent choices.\n",
    "\n",
    "**Helper Functions Used:**\n",
    "- `setup_llm_client()`: To configure the API client.\n",
    "- `get_completion()`: To send prompts to the LLM.\n",
    "- `load_artifact()`: To read the ADR template.\n",
    "- `save_artifact()`: To save the generated ADR template and the final ADR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-02 11:58:48,189 ag_aisoftdev.utils INFO LLM Client configured provider=openai model=gpt-4.1 latency_ms=None artifacts_path=None\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project's root directory to the Python path to ensure 'utils' can be imported.\n",
    "try:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "except IndexError:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from utils import setup_llm_client, get_completion, save_artifact, load_artifact, prompt_enhancer\n",
    "\n",
    "client, model_name, api_provider = setup_llm_client(model_name=\"gpt-4.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: The Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1 (Foundational): The ADR Template\n",
    "\n",
    "**Task:** A good ADR follows a consistent format. Your first task is to prompt an LLM to generate a clean, reusable ADR template in markdown.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Write a prompt that asks the LLM to generate a markdown template for an Architectural Decision Record.\n",
    "2.  The template should include sections for: `Title`, `Status` (e.g., Proposed, Accepted, Deprecated), `Context` (the problem or forces at play), `Decision` (the chosen solution), and `Consequences` (the positive and negative results of the decision).\n",
    "3.  Save the generated template to `templates/adr_template.md`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-02 12:07:44,118 ag_aisoftdev.utils INFO LLM Client configured provider=openai model=o3 latency_ms=None artifacts_path=None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating ADR Template ---\n",
      "# fictional-waffle/artifacts/adr_template.md\n",
      "```markdown\n",
      "# <Title>\n",
      "*Short, descriptive name for this architectural decision.*\n",
      "\n",
      "## Status\n",
      "*Proposed / Accepted / Deprecated / Superseded (select one and date).*\n",
      "\n",
      "## Context\n",
      "*Describe the problem, constraints, and any forces that influenced this decision.*\n",
      "\n",
      "## Decision\n",
      "*State the option chosen and give rationale for choosing it over alternatives.*\n",
      "\n",
      "## Consequences\n",
      "*List the positive and negative outcomes that result from this decision.*\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# TODO: Write a prompt to generate a markdown ADR template.\n",
    "adr_template_prompt = \"Generate a detailed markdown template for an Architecture Decision Record (ADR). The template should include \\\n",
    "    sections for Title, Status: (Proposed, Accepted, Deprecated), Context: (the problem or situation that prompted the decision),  \\\n",
    "    Consequences: (the outcomes of the decision). Provide brief descriptions for each section to guide the user on \\\n",
    "    what information to include. Format the output as a markdown document and save in fictional-waffle/artifacts/adr_template.md\"\n",
    "\n",
    "adr_template_prompt = prompt_enhancer(adr_template_prompt)\n",
    "\n",
    "print(\"--- Generating ADR Template ---\")\n",
    "adr_template_content = get_completion(adr_template_prompt, client, model_name, api_provider)\n",
    "print(adr_template_content)\n",
    "\n",
    "# Save the artifact\n",
    "if adr_template_content:\n",
    "    save_artifact(adr_template_content, \"fictional-waffle/artifacts/adr_template.md\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2 (Intermediate): AI-Assisted Research\n",
    "\n",
    "**Task:** Use the LLM to perform unbiased research on a key technical decision for our project: choosing a database for semantic search.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Write a prompt instructing the LLM to perform a technical comparison.\n",
    "2.  Ask it to compare and contrast two technical options: **\"Using PostgreSQL with the `pgvector` extension\"** versus **\"Using a specialized vector database like ChromaDB or FAISS\"**.\n",
    "3.  The prompt should ask for a balanced view for the specific use case of our new hire onboarding tool.\n",
    "4.  Store the output in a variable for the next step.\n",
    "\n",
    "> **Tip:** To get a balanced comparison, explicitly ask the LLM to 'act as an unbiased research assistant' and to list the 'pros and cons for each approach.' This prevents the model from simply recommending the more popular option and encourages a more critical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-02 12:08:48,323 ag_aisoftdev.utils INFO LLM Client configured provider=openai model=o3 latency_ms=None artifacts_path=None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Researching Database Options ---\n",
      "## Comparative Analysis\n",
      "\n",
      "| Factor | PostgreSQL + pgvector | ChromaDB / FAISS |\n",
      "|--------|----------------------|------------------|\n",
      "| **Scalability** | Vertical scaling is robust; can handle moderate vector workloads (<10M vectors) well. Horizontal scaling (sharding, replication) is mature for relational data, but vector operations may degrade in high-concurrency or high-volume scenarios. Indexing (IVFFlat, HNSW) is available, but ANN search performance lags behind specialized engines for very large datasets. | Designed for high-throughput vector workloads; FAISS is optimized for billion-scale vectors in-memory, while ChromaDB is Python-native and can scale horizontally (though production-grade clustering is still maturing). Native support for distributed architectures (especially in commercial managed offerings). Relational joins and transactional integrity are limited or absent. |\n",
      "| **Ease of Use** | Familiar setup for teams using Django/FastAPI; single database for both relational and vector data reduces operational complexity. Standard SQL queries, migrations, and ORM integration are available. Requires manual tuning for index management and query optimization. | ChromaDB offers Python-first APIs, simple setup for experimentation, and minimal boilerplate for vector operations. FAISS requires more engineering effort for persistence, deployment, and scaling. Neither is a drop-in replacement for relational databases, so a polyglot persistence strategy is needed for non-vector data. Operational maturity (backups, monitoring, upgrades) varies. |\n",
      "| **Community & Ecosystem** | PostgreSQL is highly mature, with extensive documentation and a large ecosystem. pgvector is actively maintained, widely adopted by SaaS products, and supported by major cloud providers. Many integrations (including Django ORM extensions) exist. | FAISS is well-established in ML/AI research, with strong documentation and community support among data scientists. ChromaDB is newer, with rapid development but a smaller community. Production best practices and third-party integrations are less extensive, especially for non-ML developer audiences. |\n",
      "| **Python Integration** | Full-featured Django/FastAPI support; ORMs, migrations, and admin tools work out-of-the-box. pgvector fields are supported by Django extensions. Connection pooling, async support, and transactions are mature. | ChromaDB has idiomatic Python APIs and can be embedded directly in applications. FAISS exposes Python bindings, but workflow is lower-level. Lacks out-of-the-box ORM integration; must manage data consistency and joins at the application level. Good fit for ML pipelines, less so for general web backends. |\n",
      "\n",
      "### Pros & Cons Summary\n",
      "- **PostgreSQL + pgvector**\n",
      "  - Pros: Unified storage for relational and vector data; mature scalability features for transactional workloads; strong operational tooling; seamless integration with Python web frameworks; active community and support.\n",
      "  - Cons: Vector search speed and scalability lag behind dedicated engines for very large (100M+) or high-concurrency vector workloads; tuning vector indexes may be complex; lacks advanced vector-specific features (e.g., vector sharding, auto-scaling).\n",
      "\n",
      "- **ChromaDB / FAISS**\n",
      "  - Pros: Superior vector search performance and scalability for large or high-dimensional datasets; flexible ANN algorithms and index types; Python-native APIs (ChromaDB) or high-performance C++ core (FAISS); designed for ML/AI use cases.\n",
      "  - Cons: Polyglot persistence needed for relational data; limited transactional guarantees; less mature operational tooling for typical web app needs; integration with Django/FastAPI requires custom development for data consistency and access control.\n",
      "\n",
      "### Recommendation\n",
      "\n",
      "For a SaaS web application with both relational onboarding workflows and moderate-scale semantic search, PostgreSQL with pgvector offers the most balanced, maintainable solution. It enables unified data management, leverages the team's likely familiarity with Python web frameworks, and provides sufficient vector search performance for typical SaaS use cases. Specialized vector databases (ChromaDB, FAISS) are preferable if vector workloads will dominate in scale or complexity, but they introduce higher operational and integration overhead for general-purpose web apps.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Write a prompt to research database options.\n",
    "db_research_prompt = \"Research and compare two different database solutions ('Using PostgreSQL with the pgvector extension' and \\\n",
    "                      'Using a specialized vector database like ChromaDB or FAISS') for a web application that manages user \\\n",
    "                      onboarding tasks. Consider factors such as scalability, ease of use, community support, and integration with \\\n",
    "                      Python applications. Act as an unbiased research assistant to provide a summary of the pros and cons of each \\\n",
    "                      option and recommend the best choice for this use case.\"\n",
    "\n",
    "db_research_prompt = prompt_enhancer(db_research_prompt)\n",
    "\n",
    "print(\"--- Researching Database Options ---\")\n",
    "db_research_output = get_completion(db_research_prompt, client, model_name, api_provider)\n",
    "print(db_research_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3 (Advanced): Synthesizing the ADR\n",
    "\n",
    "**Task:** Provide the LLM with your research from the previous step and have it formally document the decision.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Load the `adr_template.md` you created in the first challenge.\n",
    "2.  Create a new prompt instructing the LLM to act as a Staff Engineer.\n",
    "3.  Provide the `db_research_output` as context.\n",
    "4.  Instruct the LLM to populate the ADR template, formally documenting the decision to **use PostgreSQL with pgvector** and justifying the choice based on the synthesized pros and cons.\n",
    "5.  Save the final, completed ADR as `artifacts/adr_001_database_choice.md`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Synthesizing Final ADR ---\n",
      "```markdown\n",
      "# Vector Database Selection for Semantic Search in SaaS Web Application\n",
      "\n",
      "## Status\n",
      "Accepted – 2024-06-10\n",
      "\n",
      "## Context\n",
      "\n",
      "As part of enhancing our SaaS web application, we require a solution to support semantic search capabilities using vector similarity search. The application must manage both traditional relational data (e.g., user onboarding, workflow management) and moderate-scale vector data (e.g., text embeddings for search and recommendations). The engineering team predominantly uses Python with Django/FastAPI, and operational simplicity, reliability, and maintainability are key priorities.\n",
      "\n",
      "We evaluated two primary approaches for storing and querying vector data:\n",
      "\n",
      "1. **PostgreSQL with pgvector extension:** Enables native storage and search of vector embeddings within a standard PostgreSQL database, supporting both relational and vector use cases in a unified system.\n",
      "2. **Specialized vector databases (ChromaDB, FAISS):** Designed for high-throughput, large-scale vector search, supporting state-of-the-art approximate nearest neighbor (ANN) algorithms with horizontal scalability.\n",
      "\n",
      "Key comparative factors included scalability, ease of use and integration, operational maturity, community support, and fit with our technology stack and team expertise.\n",
      "\n",
      "## Decision\n",
      "\n",
      "We will use **PostgreSQL with the pgvector extension** as our primary data store for both relational and vector data.\n",
      "\n",
      "**Rationale:**\n",
      "\n",
      "- **Unified Architecture:** Storing both relational and vector data in the same database greatly simplifies operational overhead (backups, monitoring, scaling, security, and access control) and reduces cognitive load for the engineering team.\n",
      "- **Integration with Existing Stack:** PostgreSQL is already well-supported within our Django/FastAPI stack, with ORM support, migrations, and admin tooling readily available. pgvector is mature, actively maintained, and widely adopted.\n",
      "- **Operational Maturity:** PostgreSQL offers robust transactional guarantees, backup and recovery, and mature scaling strategies for our expected workload (<10M vectors), with broad cloud provider support.\n",
      "- **Sufficient Performance:** For our anticipated moderate-scale vector search workloads, PostgreSQL with pgvector provides acceptable performance and scalability. Advanced vector-specific features (e.g., HNSW, IVFFlat) are available for optimizing search queries.\n",
      "- **Community and Ecosystem:** Both PostgreSQL and pgvector benefit from extensive documentation, community support, and a wide range of integrations, reducing maintenance risk and enabling rapid troubleshooting.\n",
      "\n",
      "Specialized vector databases (ChromaDB/FAISS) were considered but deprioritized due to their lack of transactional guarantees, need for polyglot persistence, and higher integration and operational complexity for typical SaaS web applications.\n",
      "\n",
      "## Consequences\n",
      "\n",
      "**Positive Outcomes:**\n",
      "\n",
      "- **Simplified Operations:** One database system for both relational and vector data reduces infrastructure complexity, streamlines deployments, and unifies monitoring and backup strategies.\n",
      "- **Developer Productivity:** Familiar tools (Django ORM, migrations, admin) accelerate development and onboarding.\n",
      "- **Maintainability:** Mature ecosystem and community support minimize long-term maintenance burden and risk.\n",
      "- **Sufficient Vector Search:** For our use case (moderate dataset size, typical SaaS workloads), vector search performance is acceptable and can be tuned as needs grow.\n",
      "\n",
      "**Negative Outcomes:**\n",
      "\n",
      "- **Scalability Limitations:** For very large-scale or high-concurrency vector search workloads (e.g., >100M vectors, low-latency ML inference), PostgreSQL may not match the performance of specialized vector databases.\n",
      "- **Manual Tuning:** Indexing and query optimization for vector search may require additional expertise and maintenance.\n",
      "- **Feature Gaps:** Lacks some advanced vector-specific capabilities (e.g., auto-sharding, distributed vector search, real-time index updates) present in dedicated vector engines.\n",
      "- **Potential Migration:** If vector workloads become dominant or outgrow PostgreSQL's capabilities, migration to a specialized vector store may become necessary in the future.\n",
      "\n",
      "Overall, this decision provides the most balanced and maintainable approach for current and foreseeable product requirements, while leaving a path open for future migration to specialized solutions if needed.\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "adr_template = load_artifact(\"fictional-waffle/artifacts/adr_template.md\")\n",
    "\n",
    "# TODO: Write a prompt to synthesize the final ADR.\n",
    "synthesis_prompt = f\"Acting as a staff engineer, use the context of the {adr_template} and the previous research on database options \\\n",
    "                    in {db_research_output} to synthesize a comprehensive Architecture Decision Record (ADR). \\\n",
    "                    Ensure that the ADR clearly states the decision made, the context leading to the decision, and \\\n",
    "                    the consequences of the decision. Format the output as a markdown document.\"\n",
    "\n",
    "print(\"--- Synthesizing Final ADR ---\")\n",
    "if adr_template and 'db_research_output' in locals() and db_research_output:\n",
    "    final_adr = get_completion(synthesis_prompt, client, model_name, api_provider)\n",
    "    print(final_adr)\n",
    "    save_artifact(final_adr, \"fictional-waffle/artifacts/adr_001_database_choice.md\")\n",
    "else:\n",
    "    print(\"Skipping ADR synthesis because template or research is missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Conclusion\n",
    "\n",
    "Well done! You have used an LLM to automate a complex but critical part of the architectural process. You leveraged its vast knowledge base for research and then used it again for synthesis, turning raw analysis into a formal, structured document. This `adr_001_database_choice.md` file now serves as a permanent, valuable record for anyone who works on this project in the future.\n",
    "\n",
    "> **Key Takeaway:** The pattern of **Research -> Synthesize -> Format** is a powerful workflow. You can use an LLM to gather unstructured information and then use it again to pour that information into a structured template, creating high-quality, consistent documentation with minimal effort."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
