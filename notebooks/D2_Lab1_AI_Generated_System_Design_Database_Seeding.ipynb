{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2 - Lab 1: AI-Generated System Design & Database Seeding\n",
    "\n",
    "**Objective:** Use the PRD artifact from Day 1 to generate a detailed SQL database schema, create realistic seed data, and then use those outputs to create and seed a live, local database file.\n",
    "\n",
    "**Estimated Time:** 150 minutes\n",
    "\n",
    "**Introduction:**\n",
    "Welcome to Day 2! Today, we transition from *what* we're building to *how* we'll build it. In this lab, you will act as the lead architect for the Onboarding Tool. Your task is to use the PRD to define the data structure of the application and create a tangible database artifact that will be used for the rest of the course.\n",
    "\n",
    "For definitions of key terms used in this lab, please refer to the [GLOSSARY.md](../../GLOSSARY.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "We will load the `day1_prd.md` artifact from Day 1. This document is the primary source of truth for our project and provides the necessary context for the LLM to make intelligent design suggestions.\n",
    "\n",
    "**Model Selection:**\n",
    "Feel free to experiment with different models by changing the `model_name` in `setup_llm_client()`. Models with strong reasoning capabilities, like `gpt-4o`, `o3`, or `gemini-2.5-pro`, are excellent choices for design tasks.\n",
    "\n",
    "**Helper Functions Used:**\n",
    "- `setup_llm_client()`: To configure the API client.\n",
    "- `get_completion()`: To send prompts to the LLM.\n",
    "- `load_artifact()`: To read the PRD from the `artifacts` directory.\n",
    "- `save_artifact()`: To save the generated SQL schema and seed data.\n",
    "- `clean_llm_output()`: To remove markdown fences from the generated SQL code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-02 12:50:26,334 ag_aisoftdev.utils INFO LLM Client configured provider=openai model=gpt-4o latency_ms=None artifacts_path=None\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import sqlite3\n",
    "\n",
    "# Add the project's root directory to the Python path to ensure 'utils' can be imported.\n",
    "try:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "except IndexError:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from utils import setup_llm_client, get_completion, save_artifact, load_artifact, clean_llm_output,prompt_enhancer\n",
    "\n",
    "client, model_name, api_provider = setup_llm_client(model_name=\"gpt-4o\")\n",
    "\n",
    "# Load the PRD from Day 1\n",
    "prd_content = load_artifact(\"artifacts/waffle_PRD_output.md\")\n",
    "\n",
    "if not prd_content:\n",
    "    print(\"Warning: Could not load waffle_PRD_output.md. Lab may not function correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: The Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1 (Foundational): Generating the SQL Schema\n",
    "\n",
    "**Task:** Use the PRD to generate a normalized SQL schema for the application.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Create a prompt that instructs the LLM to act as a Database Administrator (DBA).\n",
    "2.  Provide the `prd_content` as context.\n",
    "3.  Ask the LLM to design a normalized SQL schema with at least two tables (e.g., `users` and `onboarding_tasks`).\n",
    "4.  The output should be the raw `CREATE TABLE` statements.\n",
    "5.  Save the generated SQL to `artifacts/schema.sql`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating SQL Schema ---\n",
      "create table users (\n",
      "    user_id integer primary key,\n",
      "    first_name text not null,\n",
      "    last_name text not null,\n",
      "    email text not null unique,\n",
      "    hire_date text not null,\n",
      "    role_id integer not null,\n",
      "    bio text,\n",
      "    check (email like '%@%')\n",
      ");\n",
      "\n",
      "create table user_roles (\n",
      "    role_id integer primary key,\n",
      "    role_name text not null unique\n",
      ");\n",
      "\n",
      "create index idx_users_role_id on users(role_id);\n",
      "\n",
      "alter table users add foreign key (role_id) references user_roles(role_id);\n",
      "create table users (\n",
      "    user_id integer primary key,\n",
      "    first_name text not null,\n",
      "    last_name text not null,\n",
      "    email text not null unique,\n",
      "    hire_date text not null,\n",
      "    role_id integer not null,\n",
      "    bio text,\n",
      "    check (email like '%@%')\n",
      ");\n",
      "\n",
      "create table user_roles (\n",
      "    role_id integer primary key,\n",
      "    role_name text not null unique\n",
      ");\n",
      "\n",
      "create index idx_users_role_id on users(role_id);\n",
      "\n",
      "alter table users add foreign key (role_id) references user_roles(role_id);\n"
     ]
    }
   ],
   "source": [
    "# TODO: Write a prompt to generate the SQL schema from the PRD.\n",
    "schema_prompt = f\"\"\"\n",
    "You are a Senior Database Architect. Design a comprehensive, production-ready SQL database schema based on the Product Requirements Document (PRD) provided below.\n",
    "\n",
    "**Requirements:**\n",
    "- Create not more than 2 tables (e.g., users and user_roles)\n",
    "- Include all necessary tables to fully support the application's features and workflows\n",
    "- Define appropriate columns with proper data types for each table\n",
    "- Establish primary keys for all tables\n",
    "- Define foreign keys and relationships between tables where applicable\n",
    "- Follow database normalization best practices (at least 3NF)\n",
    "- Use standard SQL naming conventions (lowercase with underscores)\n",
    "- Include appropriate indexes for performance optimization\n",
    "- Add constraints (NOT NULL, UNIQUE, CHECK) where logically necessary\n",
    "- Consider data integrity, scalability, and query performance\n",
    "- Use AUTO_INCREMENT for primary keys; instead\n",
    "- Use SQL Lite compatible syntax only\n",
    "\n",
    "**Output Format:**\n",
    "Provide ONLY the raw SQL CREATE TABLE statements without:\n",
    "- Markdown code blocks or formatting\n",
    "- Explanatory text or comments\n",
    "- Additional documentation\n",
    "\n",
    "Each CREATE TABLE statement should be complete and executable.\n",
    "\n",
    "**PRD:**\n",
    "{prd_content}\n",
    "\"\"\"\n",
    "\n",
    "#enhanced_schema_prompt = prompt_enhancer(schema_prompt)\n",
    "print(\"--- Generating SQL Schema ---\")\n",
    "if prd_content:\n",
    "    generated_schema = get_completion(schema_prompt, client, model_name, api_provider)\n",
    "    print(generated_schema)\n",
    "    \n",
    "    # Clean up the generated schema using our helper function\n",
    "    cleaned_schema = clean_llm_output(generated_schema, language='sql')\n",
    "    print(cleaned_schema)\n",
    "    \n",
    "    # Save the cleaned schema\n",
    "    save_artifact(cleaned_schema, 'artifacts/waffle_schema.sql',overwrite=True)\n",
    "else:\n",
    "    print(\"Skipping schema generation because PRD is missing.\")\n",
    "    cleaned_schema = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2 (Intermediate): Generating Realistic Seed Data\n",
    "\n",
    "**Task:** Prompt the LLM to generate realistic seed data that conforms to the schema you just created.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Create a new prompt.\n",
    "2.  Provide both the `prd_content` and the `cleaned_schema` as context.\n",
    "3.  Instruct the LLM to generate 5-10 realistic `INSERT` statements for your tables.\n",
    "4.  The data should be relevant to a new hire onboarding tool (e.g., sample user names and task titles like \"Complete HR Paperwork\").\n",
    "5.  Save the generated `INSERT` statements to `artifacts/seed_data.sql`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating Seed Data ---\n",
      "insert into user_roles (role_id, role_name) values (1, 'Software Engineer');\n",
      "insert into user_roles (role_id, role_name) values (2, 'HR Manager');\n",
      "insert into user_roles (role_id, role_name) values (3, 'Product Manager');\n",
      "insert into user_roles (role_id, role_name) values (4, 'Design Lead');\n",
      "insert into user_roles (role_id, role_name) values (5, 'Sales Executive');\n",
      "insert into user_roles (role_id, role_name) values (6, 'Customer Support');\n",
      "insert into user_roles (role_id, role_name) values (7, 'Marketing Specialist');\n",
      "insert into user_roles (role_id, role_name) values (8, 'DevOps Engineer');\n",
      "insert into user_roles (role_id, role_name) values (9, 'Data Analyst');\n",
      "insert into user_roles (role_id, role_name) values (10, 'Finance Officer');\n",
      "\n",
      "insert into users (user_id, first_name, last_name, email, hire_date, role_id, bio) values (1, 'Sarah', 'Johnson', 'sarah.johnson@company.com', '2023-01-15', 1, 'Experienced software engineer with a passion for developing innovative programs.');\n",
      "insert into users (user_id, first_name, last_name, email, hire_date, role_id, bio) values (2, 'Michael', 'Chen', 'michael.chen@company.com', '2023-02-20', 2, 'HR manager with a strong background in employee relations and talent acquisition.');\n",
      "insert into users (user_id, first_name, last_name, email, hire_date, role_id, bio) values (3, 'Emily', 'Smith', 'emily.smith@company.com', '2023-03-10', 3, 'Product manager with expertise in leading cross-functional teams to deliver innovative products.');\n",
      "insert into users (user_id, first_name, last_name, email, hire_date, role_id, bio) values (4, 'James', 'Brown', 'james.brown@company.com', '2023-01-25', 4, 'Creative design lead with a proven track record in brand development and digital media.');\n",
      "insert into users (user_id, first_name, last_name, email, hire_date, role_id, bio) values (5, 'Linda', 'Davis', 'linda.davis@company.com', '2023-04-05', 5, 'Sales executive with a passion for building lasting client relationships and exceeding sales targets.');\n",
      "insert into users (user_id, first_name, last_name, email, hire_date, role_id, bio) values (6, 'John', 'Wilson', 'john.wilson@company.com', '2023-02-15', 6, 'Customer support specialist with a focus on enhancing customer satisfaction and retention.');\n",
      "insert into users (user_id, first_name, last_name, email, hire_date, role_id, bio) values (7, 'Barbara', 'Taylor', 'barbara.taylor@company.com', '2023-03-20', 7, 'Marketing specialist with a knack for crafting compelling campaigns that drive engagement.');\n",
      "insert into users (user_id, first_name, last_name, email, hire_date, role_id, bio) values (8, 'Robert', 'Anderson', 'robert.anderson@company.com', '2023-04-10', 8, 'DevOps engineer with expertise in cloud services and automation.');\n",
      "insert into users (user_id, first_name, last_name, email, hire_date, role_id, bio) values (9, 'Patricia', 'Martinez', 'patricia.martinez@company.com', '2023-01-30', 9, 'Data analyst with a strong analytical mind and a passion for data-driven decision making.');\n",
      "insert into users (user_id, first_name, last_name, email, hire_date, role_id, bio) values (10, 'Charles', 'Hernandez', 'charles.hernandez@company.com', '2023-02-25', 10, 'Finance officer with a solid background in corporate finance and financial planning.');\n"
     ]
    }
   ],
   "source": [
    "# TODO: Write a prompt to generate realistic seed data.\n",
    "seed_data_prompt = f\"\"\"\n",
    "You are a Database Analyst specializing in test data generation. Your task is to create realistic seed data that is STRICTLY COMPLIANT with the provided database schema.\n",
    "\n",
    "**Critical Requirements:**\n",
    "- Generate exactly 10 INSERT statements for EACH table defined in the schema\n",
    "- ALL data must conform to the exact data types specified in the schema (VARCHAR lengths, INTEGER ranges, DATE formats, etc.)\n",
    "- STRICTLY respect all constraints defined in the schema:\n",
    "  * PRIMARY KEY constraints (ensure uniqueness)\n",
    "  * FOREIGN KEY constraints (ensure referential integrity - all FK values must reference existing PK values)\n",
    "  * NOT NULL constraints (provide values for ALL NOT NULL columns)\n",
    "  * UNIQUE constraints (ensure no duplicates)\n",
    "  * CHECK constraints (satisfy all validation rules)\n",
    "  * DEFAULT values (you may use them or override with explicit values)\n",
    "- Maintain proper insert order: parent tables BEFORE child tables to satisfy FK dependencies\n",
    "- Use realistic, contextually appropriate data for a new hire onboarding tool\n",
    "\n",
    "**Data Guidelines:**\n",
    "- User names: realistic full names (e.g., \"Sarah Johnson\", \"Michael Chen\")\n",
    "- Task titles: relevant onboarding activities (e.g., \"Complete I-9 Form\", \"Setup Workstation\", \"Attend Security Training\")\n",
    "- Dates: use logical sequences (hire dates before task due dates, etc.)\n",
    "- Email addresses: follow standard format (firstname.lastname@company.com)\n",
    "- Status values: match any ENUM or CHECK constraint definitions exactly\n",
    "- IDs: start from 1 and increment sequentially\n",
    "\n",
    "**Output Format:**\n",
    "Provide ONLY executable SQL INSERT statements:\n",
    "- No markdown formatting or code blocks\n",
    "- No explanatory comments or documentation\n",
    "- One INSERT statement per line or proper multi-row INSERT syntax\n",
    "- Statements must be ready to execute immediately after CREATE TABLE statements\n",
    "\n",
    "**Context:**\n",
    "PRD: {prd_content}\n",
    "Schema: {cleaned_schema}\n",
    "\n",
    "**Validation Checklist (verify before output):**\n",
    "□ All FK values reference existing PK values\n",
    "□ All NOT NULL columns have values\n",
    "□ All data types match schema exactly\n",
    "□ All string lengths within VARCHAR limits\n",
    "□ Proper insert order maintained\n",
    "□ All constraints satisfied\n",
    "\"\"\"\n",
    "#enhanced_seed_data_prompt = prompt_enhancer(seed_data_prompt)\n",
    "\n",
    "print(\"--- Generating Seed Data ---\")\n",
    "if prd_content and cleaned_schema:\n",
    "    generated_seed_data = get_completion(seed_data_prompt, client, model_name, api_provider)\n",
    "    \n",
    "    # Clean up the generated seed data\n",
    "    cleaned_seed_data = clean_llm_output(generated_seed_data, language='sql')\n",
    "    print(cleaned_seed_data)\n",
    "    \n",
    "    # Save the cleaned seed data\n",
    "    save_artifact(cleaned_seed_data, 'artifacts/waffle_seed_data.sql',overwrite=True)\n",
    "else:\n",
    "    print(\"Skipping seed data generation because PRD or schema is missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3 (Advanced): Creating and Seeding a Live Database\n",
    "\n",
    "**Task:** This is a critical technical step. You will write a Python script to execute the generated SQL files, creating a live `onboarding.db` file that your application will use.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Complete the `create_database` function below.\n",
    "2.  The function should first connect to (and thus create) a SQLite database file named `artifacts/onboarding.db`.\n",
    "3.  It should then open and execute the `schema.sql` file to create the tables.\n",
    "4.  Finally, it should open and execute the `seed_data.sql` file to populate the tables.\n",
    "5.  Use a `try...finally` block to ensure the database connection is always closed, even if an error occurs.\n",
    "\n",
    "> **Hint:** The `try...finally` block is a crucial Python pattern. The code in the `finally` block will run whether the `try` block succeeds or fails, making it the perfect place to ensure resources like database connections are always closed.\n",
    "\n",
    "**Expected Quality:** A physical `onboarding.db` file in your `artifacts` folder. This is a tangible asset that proves your design is valid and provides a concrete foundation for backend development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def tables_to_drop(schema_path):\n",
    "    \"\"\"Extracts all table names from CREATE TABLE statements in the schema file.\"\"\"\n",
    "    if not os.path.exists(schema_path):\n",
    "        print(f\"Error: Schema file not found at {schema_path}\")\n",
    "        return []\n",
    "    with open(schema_path, 'r', encoding='utf-8') as f:\n",
    "        sql = f.read()\n",
    "    # Regex to match CREATE TABLE [IF NOT EXISTS] table_name\n",
    "    pattern = r\"CREATE TABLE(?: IF NOT EXISTS)?\\s+([a-zA-Z0-9_]+)\"\n",
    "    tables = re.findall(pattern, sql, re.IGNORECASE)\n",
    "    return tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to database at c:\\Users\\labadmin\\Desktop\\Repository\\AG-AISOFTDEV\\artifacts\\waffle_tech_suite.db\n",
      "Checking for existing tables in database...\n",
      "Found 2 existing tables: ['users', 'user_roles']\n",
      "  Dropped table: users\n",
      "  Dropped table: user_roles\n",
      "All existing tables dropped successfully.\n",
      "Schema-based table cleanup completed.\n",
      "Tables created successfully.\n",
      "Seed data inserted successfully.\n",
      "Database changes committed.\n",
      "Database connection closed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def create_database(db_path, schema_path, seed_path):\n",
    "    \"\"\"Creates and seeds a SQLite database from SQL files.\"\"\"\n",
    "    if not os.path.exists(schema_path):\n",
    "        print(f\"Error: Schema file not found at {schema_path}\")\n",
    "        return\n",
    "    \n",
    "    conn = None\n",
    "    try:\n",
    "        # Connect to the SQLite database. This will create the file if it doesn't exist.\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        cursor = conn.cursor()\n",
    "        print(f\"Successfully connected to database at {db_path}\")\n",
    "\n",
    "        # --- Drop ALL existing tables in the database ---\n",
    "        print(\"Checking for existing tables in database...\")\n",
    "        cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%';\")\n",
    "        existing_tables = cursor.fetchall()\n",
    "        \n",
    "        if existing_tables:\n",
    "            print(f\"Found {len(existing_tables)} existing tables: {[table[0] for table in existing_tables]}\")\n",
    "            # Disable foreign key constraints temporarily to allow dropping tables\n",
    "            cursor.execute(\"PRAGMA foreign_keys = OFF;\")\n",
    "            \n",
    "            for table in existing_tables:\n",
    "                table_name = table[0]\n",
    "                cursor.execute(f\"DROP TABLE IF EXISTS {table_name};\")\n",
    "                print(f\"  Dropped table: {table_name}\")\n",
    "            \n",
    "            # Re-enable foreign key constraints\n",
    "            cursor.execute(\"PRAGMA foreign_keys = ON;\")\n",
    "            print(\"All existing tables dropped successfully.\")\n",
    "        else:\n",
    "            print(\"No existing tables found in database.\")\n",
    "\n",
    "        # --- Additional safety: Drop tables from schema.sql if any remain ---\n",
    "        tables_from_schema = tables_to_drop(schema_path)\n",
    "        for table in tables_from_schema:\n",
    "            cursor.execute(f\"DROP TABLE IF EXISTS {table};\")\n",
    "        print(\"Schema-based table cleanup completed.\")\n",
    "\n",
    "        # Read the content of the schema file using load_artifact.\n",
    "        schema_sql = load_artifact(schema_path)\n",
    "        \n",
    "        # Replace TIMESTAMP WITH TIME ZONE with TIMESTAMP for SQLite compatibility\n",
    "        if schema_sql:\n",
    "            schema_sql = schema_sql.replace(\"TIMESTAMP WITH TIME ZONE\", \"TIMESTAMP\")\n",
    "            cursor.executescript(schema_sql)\n",
    "            print(\"Tables created successfully.\")\n",
    "\n",
    "        # Check if the seed data file exists. If it does, load and execute it.\n",
    "        if os.path.exists(seed_path):\n",
    "            seed_sql = load_artifact(seed_path)\n",
    "            if seed_sql:\n",
    "                cursor.executescript(seed_sql)\n",
    "                print(\"Seed data inserted successfully.\")\n",
    "\n",
    "        # Commit the changes to the database.\n",
    "        conn.commit()\n",
    "        print(\"Database changes committed.\")    \n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Database error: {e}\")\n",
    "    finally:\n",
    "        # Ensure the connection is closed if it was opened.\n",
    "        if conn:\n",
    "            conn.close()\n",
    "            print(\"Database connection closed.\")\n",
    "\n",
    "def query_table(db_path, query, params=None):\n",
    "    if not os.path.exists(db_path):\n",
    "        print(f\"Error: Database file not found at {db_path}\")\n",
    "        return None\n",
    "    conn = None\n",
    "    try:\n",
    "        # Connect to the SQLite database\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        cursor = conn.cursor()\n",
    "        # Execute the query with or without parameters\n",
    "        if params:\n",
    "            cursor.execute(query, params)\n",
    "        else:\n",
    "            cursor.execute(query)\n",
    "        # Fetch all results\n",
    "        results = cursor.fetchall()\n",
    "        # Get column names for better readability (optional)\n",
    "        column_names = [description[0] for description in cursor.description]\n",
    "        print(f\"Query executed successfully. Found {len(results)} rows.\")\n",
    "        print(f\"Columns: {', '.join(column_names)}\")\n",
    "        return results\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Database query error: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        # Ensure the connection is closed if it was opened\n",
    "        if conn:\n",
    "            conn.close()\n",
    "try:\n",
    "    project_root\n",
    "except NameError:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "\n",
    "db_file = os.path.join(project_root, \"artifacts\", \"waffle_tech_suite.db\")\n",
    "schema_file = os.path.join(project_root, \"artifacts\", \"waffle_schema.sql\")\n",
    "seed_file = os.path.join(project_root, \"artifacts\", \"waffle_seed_data.sql\")\n",
    "\n",
    "# Execute the function\n",
    "create_database(db_file, schema_file, seed_file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Database Verification ===\n",
      "\n",
      "--- User Roles ---\n",
      "Query executed successfully. Found 10 rows.\n",
      "Columns: role_id, role_name\n",
      "Role ID: 1, Role Name: Software Engineer\n",
      "Role ID: 2, Role Name: HR Manager\n",
      "Role ID: 3, Role Name: Product Manager\n",
      "Role ID: 4, Role Name: Design Lead\n",
      "Role ID: 5, Role Name: Sales Executive\n",
      "Role ID: 6, Role Name: Customer Support\n",
      "Role ID: 7, Role Name: Marketing Specialist\n",
      "Role ID: 8, Role Name: DevOps Engineer\n",
      "Role ID: 9, Role Name: Data Analyst\n",
      "Role ID: 10, Role Name: Finance Officer\n",
      "\n",
      "--- Users ---\n",
      "Query executed successfully. Found 10 rows.\n",
      "Columns: user_id, first_name, last_name, email, hire_date, role_id, bio\n",
      "User ID: 1, Name: Sarah Johnson, Email: sarah.johnson@company.com, Role ID: 1\n",
      "User ID: 2, Name: Michael Chen, Email: michael.chen@company.com, Role ID: 2\n",
      "User ID: 3, Name: Emily Smith, Email: emily.smith@company.com, Role ID: 3\n",
      "User ID: 4, Name: James Brown, Email: james.brown@company.com, Role ID: 4\n",
      "User ID: 5, Name: Linda Davis, Email: linda.davis@company.com, Role ID: 5\n",
      "User ID: 6, Name: John Wilson, Email: john.wilson@company.com, Role ID: 6\n",
      "User ID: 7, Name: Barbara Taylor, Email: barbara.taylor@company.com, Role ID: 7\n",
      "User ID: 8, Name: Robert Anderson, Email: robert.anderson@company.com, Role ID: 8\n",
      "User ID: 9, Name: Patricia Martinez, Email: patricia.martinez@company.com, Role ID: 9\n",
      "User ID: 10, Name: Charles Hernandez, Email: charles.hernandez@company.com, Role ID: 10\n",
      "\n",
      "--- Users with Role Names ---\n",
      "Query executed successfully. Found 10 rows.\n",
      "Columns: user_id, first_name, last_name, email, role_name\n",
      "User: Sarah Johnson (sarah.johnson@company.com) - Role: Software Engineer\n",
      "User: Michael Chen (michael.chen@company.com) - Role: HR Manager\n",
      "User: Emily Smith (emily.smith@company.com) - Role: Product Manager\n",
      "User: James Brown (james.brown@company.com) - Role: Design Lead\n",
      "User: Linda Davis (linda.davis@company.com) - Role: Sales Executive\n",
      "User: John Wilson (john.wilson@company.com) - Role: Customer Support\n",
      "User: Barbara Taylor (barbara.taylor@company.com) - Role: Marketing Specialist\n",
      "User: Robert Anderson (robert.anderson@company.com) - Role: DevOps Engineer\n",
      "User: Patricia Martinez (patricia.martinez@company.com) - Role: Data Analyst\n",
      "User: Charles Hernandez (charles.hernandez@company.com) - Role: Finance Officer\n"
     ]
    }
   ],
   "source": [
    "# Verify the database was created and populated correctly\n",
    "print(\"=== Database Verification ===\")\n",
    "\n",
    "# Query user_roles table\n",
    "print(\"\\n--- User Roles ---\")\n",
    "roles_results = query_table(db_file, \"SELECT * FROM user_roles ORDER BY role_id\")\n",
    "if roles_results:\n",
    "    for row in roles_results:\n",
    "        print(f\"Role ID: {row[0]}, Role Name: {row[1]}\")\n",
    "\n",
    "# Query users table\n",
    "print(\"\\n--- Users ---\")\n",
    "users_results = query_table(db_file, \"SELECT * FROM users ORDER BY user_id\")\n",
    "if users_results:\n",
    "    for row in users_results:\n",
    "        print(f\"User ID: {row[0]}, Name: {row[1]} {row[2]}, Email: {row[3]}, Role ID: {row[5]}\")\n",
    "\n",
    "# Query with JOIN to show user roles\n",
    "print(\"\\n--- Users with Role Names ---\")\n",
    "join_results = query_table(db_file, \"\"\"\n",
    "    SELECT u.user_id, u.first_name, u.last_name, u.email, ur.role_name \n",
    "    FROM users u \n",
    "    JOIN user_roles ur ON u.role_id = ur.role_id \n",
    "    ORDER BY u.user_id\n",
    "\"\"\")\n",
    "if join_results:\n",
    "    for row in join_results:\n",
    "        print(f\"User: {row[1]} {row[2]} ({row[3]}) - Role: {row[4]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Lab Completion Status ===\n",
      "✅ Database file created: True\n",
      "✅ Schema file exists: True\n",
      "✅ Seed data file exists: True\n",
      "✅ Database contains 10 users\n",
      "✅ Database contains 10 user roles\n",
      "\n",
      "🎉 Lab completed successfully!\n",
      "Database location: c:\\Users\\labadmin\\Desktop\\Repository\\AG-AISOFTDEV\\artifacts\\waffle_tech_suite.db\n"
     ]
    }
   ],
   "source": [
    "# Final status check\n",
    "import os\n",
    "\n",
    "print(\"=== Lab Completion Status ===\")\n",
    "print(f\"✅ Database file created: {os.path.exists(db_file)}\")\n",
    "print(f\"✅ Schema file exists: {os.path.exists(schema_file)}\")\n",
    "print(f\"✅ Seed data file exists: {os.path.exists(seed_file)}\")\n",
    "print(f\"✅ Database contains {len(users_results)} users\")\n",
    "print(f\"✅ Database contains {len(roles_results)} user roles\")\n",
    "print(f\"\\n🎉 Lab completed successfully!\")\n",
    "print(f\"Database location: {db_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Conclusion\n",
    "\n",
    "Excellent work! You have now moved from abstract requirements to a concrete, physical database artifact. You've used an LLM to design a schema, generate realistic test data, and then used a Python script to bring that database to life. This `onboarding.db` file is the foundation upon which we will build our API in Day 3.\n",
    "\n",
    "> **Key Takeaway:** The ability to generate structured data definitions (like a SQL schema) from unstructured text (like a PRD) is a core skill in AI-assisted development. It automates a critical and often time-consuming design step."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
