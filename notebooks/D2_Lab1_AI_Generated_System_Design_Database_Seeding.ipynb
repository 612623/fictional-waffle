{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2 - Lab 1: AI-Generated System Design & Database Seeding\n",
    "\n",
    "**Objective:** Use the PRD artifact from Day 1 to generate a detailed SQL database schema, create realistic seed data, and then use those outputs to create and seed a live, local database file.\n",
    "\n",
    "**Estimated Time:** 150 minutes\n",
    "\n",
    "**Introduction:**\n",
    "Welcome to Day 2! Today, we transition from *what* we're building to *how* we'll build it. In this lab, you will act as the lead architect for the Onboarding Tool. Your task is to use the PRD to define the data structure of the application and create a tangible database artifact that will be used for the rest of the course.\n",
    "\n",
    "For definitions of key terms used in this lab, please refer to the [GLOSSARY.md](../../GLOSSARY.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "We will load the `day1_prd.md` artifact from Day 1. This document is the primary source of truth for our project and provides the necessary context for the LLM to make intelligent design suggestions.\n",
    "\n",
    "**Model Selection:**\n",
    "Feel free to experiment with different models by changing the `model_name` in `setup_llm_client()`. Models with strong reasoning capabilities, like `gpt-4o`, `o3`, or `gemini-2.5-pro`, are excellent choices for design tasks.\n",
    "\n",
    "**Helper Functions Used:**\n",
    "- `setup_llm_client()`: To configure the API client.\n",
    "- `get_completion()`: To send prompts to the LLM.\n",
    "- `load_artifact()`: To read the PRD from the `artifacts` directory.\n",
    "- `save_artifact()`: To save the generated SQL schema and seed data.\n",
    "- `clean_llm_output()`: To remove markdown fences from the generated SQL code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-02 14:07:19,608 ag_aisoftdev.utils INFO LLM Client configured provider=openai model=gpt-4o latency_ms=None artifacts_path=None\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import sqlite3\n",
    "\n",
    "# Add the project's root directory to the Python path to ensure 'utils' can be imported.\n",
    "try:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "except IndexError:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from utils import setup_llm_client, get_completion, save_artifact, load_artifact, clean_llm_output,prompt_enhancer\n",
    "\n",
    "client, model_name, api_provider = setup_llm_client(model_name=\"gpt-4o\")\n",
    "\n",
    "# Load the PRD from Day 1\n",
    "prd_content = load_artifact(\"artifacts/waffle_PRD_output.md\")\n",
    "\n",
    "if not prd_content:\n",
    "    print(\"Warning: Could not load waffle_PRD_output.md. Lab may not function correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: The Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1 (Foundational): Generating the SQL Schema\n",
    "\n",
    "**Task:** Use the PRD to generate a normalized SQL schema for the application.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Create a prompt that instructs the LLM to act as a Database Administrator (DBA).\n",
    "2.  Provide the `prd_content` as context.\n",
    "3.  Ask the LLM to design a normalized SQL schema with at least two tables (e.g., `users` and `onboarding_tasks`).\n",
    "4.  The output should be the raw `CREATE TABLE` statements.\n",
    "5.  Save the generated SQL to `artifacts/schema.sql`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating SQL Schema ---\n",
      "CREATE TABLE users (\n",
      "    user_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
      "    first_name TEXT NOT NULL,\n",
      "    last_name TEXT NOT NULL,\n",
      "    email TEXT NOT NULL UNIQUE,\n",
      "    password_hash TEXT NOT NULL,\n",
      "    role_id INTEGER NOT NULL,\n",
      "    start_date DATE NOT NULL,\n",
      "    FOREIGN KEY (role_id) REFERENCES user_roles(role_id)\n",
      ");\n",
      "\n",
      "CREATE TABLE user_roles (\n",
      "    role_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
      "    role_name TEXT NOT NULL UNIQUE\n",
      ");\n",
      "CREATE TABLE users (\n",
      "    user_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
      "    first_name TEXT NOT NULL,\n",
      "    last_name TEXT NOT NULL,\n",
      "    email TEXT NOT NULL UNIQUE,\n",
      "    password_hash TEXT NOT NULL,\n",
      "    role_id INTEGER NOT NULL,\n",
      "    start_date DATE NOT NULL,\n",
      "    FOREIGN KEY (role_id) REFERENCES user_roles(role_id)\n",
      ");\n",
      "\n",
      "CREATE TABLE user_roles (\n",
      "    role_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
      "    role_name TEXT NOT NULL UNIQUE\n",
      ");\n",
      "CREATE TABLE users (\n",
      "    user_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
      "    first_name TEXT NOT NULL,\n",
      "    last_name TEXT NOT NULL,\n",
      "    email TEXT NOT NULL UNIQUE,\n",
      "    password_hash TEXT NOT NULL,\n",
      "    role_id INTEGER NOT NULL,\n",
      "    start_date DATE NOT NULL,\n",
      "    FOREIGN KEY (role_id) REFERENCES user_roles(role_id)\n",
      ");\n",
      "\n",
      "CREATE TABLE user_roles (\n",
      "    role_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
      "    role_name TEXT NOT NULL UNIQUE\n",
      ");\n",
      "CREATE TABLE users (\n",
      "    user_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
      "    first_name TEXT NOT NULL,\n",
      "    last_name TEXT NOT NULL,\n",
      "    email TEXT NOT NULL UNIQUE,\n",
      "    password_hash TEXT NOT NULL,\n",
      "    role_id INTEGER NOT NULL,\n",
      "    start_date DATE NOT NULL,\n",
      "    FOREIGN KEY (role_id) REFERENCES user_roles(role_id)\n",
      ");\n",
      "\n",
      "CREATE TABLE user_roles (\n",
      "    role_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
      "    role_name TEXT NOT NULL UNIQUE\n",
      ");\n"
     ]
    }
   ],
   "source": [
    "# TODO: Write a prompt to generate the SQL schema from the PRD.\n",
    "schema_prompt = f\"\"\"\n",
    "You are a Senior Database Architect. Design a comprehensive, production-ready SQL database schema based on the Product Requirements Document (PRD) provided below.\n",
    "\n",
    "**Requirements:**\n",
    "- Create not more than 2 tables (e.g., users and user_roles)\n",
    "- Include all necessary tables to fully support the application's features and workflows\n",
    "- Define appropriate columns with proper data types for each table\n",
    "- Establish primary keys for all tables\n",
    "- Define foreign keys and relationships between tables where applicable\n",
    "- Follow database normalization best practices (at least 3NF)\n",
    "- Use standard SQL naming conventions (lowercase with underscores)\n",
    "- Include appropriate indexes for performance optimization\n",
    "- Add constraints (NOT NULL, UNIQUE, CHECK) where logically necessary\n",
    "- Consider data integrity, scalability, and query performance\n",
    "- Use AUTO_INCREMENT for primary keys; instead\n",
    "- Use SQL Lite compatible syntax only\n",
    "\n",
    "**Output Format:**\n",
    "Provide ONLY the raw SQL CREATE TABLE statements without:\n",
    "- Markdown code blocks or formatting\n",
    "- Explanatory text or comments\n",
    "- Additional documentation\n",
    "\n",
    "Each CREATE TABLE statement should be complete and executable.\n",
    "\n",
    "**PRD:**\n",
    "{prd_content}\n",
    "\"\"\"\n",
    "\n",
    "#enhanced_schema_prompt = prompt_enhancer(schema_prompt)\n",
    "print(\"--- Generating SQL Schema ---\")\n",
    "if prd_content:\n",
    "    generated_schema = get_completion(schema_prompt, client, model_name, api_provider)\n",
    "    print(generated_schema)\n",
    "    \n",
    "    # Clean up the generated schema using our helper function\n",
    "    cleaned_schema = clean_llm_output(generated_schema, language='sql')\n",
    "    print(cleaned_schema)\n",
    "    \n",
    "    # Save the cleaned schema\n",
    "    save_artifact(cleaned_schema, 'artifacts/waffle_schema.sql',overwrite=True)\n",
    "else:\n",
    "    print(\"Skipping schema generation because PRD is missing.\")\n",
    "    cleaned_schema = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2 (Intermediate): Generating Realistic Seed Data\n",
    "\n",
    "**Task:** Prompt the LLM to generate realistic seed data that conforms to the schema you just created.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Create a new prompt.\n",
    "2.  Provide both the `prd_content` and the `cleaned_schema` as context.\n",
    "3.  Instruct the LLM to generate 5-10 realistic `INSERT` statements for your tables.\n",
    "4.  The data should be relevant to a new hire onboarding tool (e.g., sample user names and task titles like \"Complete HR Paperwork\").\n",
    "5.  Save the generated `INSERT` statements to `artifacts/seed_data.sql`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating Seed Data ---\n",
      "INSERT INTO user_roles (role_id, role_name) VALUES\n",
      "(1, 'Software Engineer'),\n",
      "(2, 'HR Manager'),\n",
      "(3, 'Product Manager'),\n",
      "(4, 'Designer'),\n",
      "(5, 'Data Analyst'),\n",
      "(6, 'Marketing Specialist'),\n",
      "(7, 'Sales Representative'),\n",
      "(8, 'Technical Support'),\n",
      "(9, 'Finance Manager'),\n",
      "(10, 'Operations Manager');\n",
      "\n",
      "INSERT INTO users (user_id, first_name, last_name, email, hire_date, role_id, bio) VALUES\n",
      "(1, 'John', 'Doe', 'john.doe@company.com', '2023-01-15', 1, 'A passionate software engineer.'),\n",
      "(2, 'Jane', 'Smith', 'jane.smith@company.com', '2023-02-01', 2, 'Experienced HR manager.'),\n",
      "(3, 'Emily', 'Johnson', 'emily.johnson@company.com', '2023-03-10', 3, 'Product manager with a focus on innovation.'),\n",
      "(4, 'Michael', 'Brown', 'michael.brown@company.com', '2023-04-20', 4, 'Creative designer with a knack for aesthetics.'),\n",
      "(5, 'Jessica', 'Williams', 'jessica.williams@company.com', '2023-05-05', 5, 'Data analyst who loves numbers.'),\n",
      "(6, 'David', 'Jones', 'david.jones@company.com', '2023-06-15', 6, 'Marketing specialist with creative strategies.'),\n",
      "(7, 'Sarah', 'Miller', 'sarah.miller@company.com', '2023-07-20', 7, 'Sales representative with excellent communication.'),\n",
      "(8, 'Daniel', 'Davis', 'daniel.davis@company.com', '2023-08-25', 8, 'Technical support expert.'),\n",
      "(9, 'Laura', 'Garcia', 'laura.garcia@company.com', '2023-09-05', 9, 'Finance manager with a keen eye for detail.'),\n",
      "(10, 'James', 'Martinez', 'james.martinez@company.com', '2023-10-10', 10, 'Operations manager with efficient processes.'),\n",
      "(11, 'Sophia', 'Lopez', 'sophia.lopez@company.com', '2023-01-18', 1, 'Software engineer focused on backend.'),\n",
      "(12, 'Oliver', 'Wilson', 'oliver.wilson@company.com', '2023-02-05', 2, 'HR manager passionate about employee engagement.'),\n",
      "(13, 'Isabella', 'Anderson', 'isabella.anderson@company.com', '2023-03-12', 3, 'Product manager with a user-centric approach.'),\n",
      "(14, 'Ethan', 'Thomas', 'ethan.thomas@company.com', '2023-04-22', 4, 'Designer with a focus on user experience.'),\n",
      "(15, 'Mia', 'Taylor', 'mia.taylor@company.com', '2023-05-08', 5, 'Data analyst with strong statistical skills.'),\n",
      "(16, 'Lucas', 'Moore', 'lucas.moore@company.com', '2023-06-18', 6, 'Marketing specialist with digital expertise.'),\n",
      "(17, 'Amelia', 'Jackson', 'amelia.jackson@company.com', '2023-07-22', 7, 'Sales representative with a customer-first approach.'),\n",
      "(18, 'Mason', 'Martin', 'mason.martin@company.com', '2023-08-28', 8, 'Technical support specialist with problem-solving skills.'),\n",
      "(19, 'Charlotte', 'Lee', 'charlotte.lee@company.com', '2023-09-08', 9, 'Finance manager with strategic thinking.'),\n",
      "(20, 'Elijah', 'Perez', 'elijah.perez@company.com', '2023-10-15', 10, 'Operations manager focused on efficiency.'),\n",
      "(21, 'Ava', 'White', 'ava.white@company.com', '2023-01-22', 1, 'Software engineer specializing in frontend.'),\n",
      "(22, 'William', 'Harris', 'william.harris@company.com', '2023-02-10', 2, 'HR manager with a focus on recruitment.'),\n",
      "(23, 'Sophia', 'Clark', 'sophia.clark@company.com', '2023-03-15', 3, 'Product manager with a tech-savvy approach.'),\n",
      "(24, 'Logan', 'Lewis', 'logan.lewis@company.com', '2023-04-25', 4, 'Designer with a love for graphic design.'),\n",
      "(25, 'Aiden', 'Walker', 'aiden.walker@company.com', '2023-05-12', 5, 'Data analyst with a focus on predictive analytics.'),\n",
      "(26, 'Harper', 'Hall', 'harper.hall@company.com', '2023-06-20', 6, 'Marketing specialist with a focus on branding.'),\n",
      "(27, 'Alexander', 'Young', 'alexander.young@company.com', '2023-07-25', 7, 'Sales representative with a focus on B2B sales.'),\n",
      "(28, 'Liam', 'King', 'liam.king@company.com', '2023-08-30', 8, 'Technical support with a focus on software issues.'),\n",
      "(29, 'Abigail', 'Wright', 'abigail.wright@company.com', '2023-09-10', 9, 'Finance manager with a strong accounting background.'),\n",
      "(30, 'Matthew', 'Scott', 'matthew.scott@company.com', '2023-10-18', 10, 'Operations manager with a focus on logistics.'),\n",
      "(31, 'Emma', 'Green', 'emma.green@company.com', '2023-01-25', 1, 'Software engineer with a passion for code.'),\n",
      "(32, 'Jayden', 'Baker', 'jayden.baker@company.com', '2023-02-15', 2, 'HR manager focused on employee retention.'),\n",
      "(33, 'Chloe', 'Adams', 'chloe.adams@company.com', '2023-03-18', 3, 'Product manager with agile methodology expertise.'),\n",
      "(34, 'Noah', 'Nelson', 'noah.nelson@company.com', '2023-04-28', 4, 'Designer with a strong portfolio.'),\n",
      "(35, 'Zoe', 'Carter', 'zoe.carter@company.com', '2023-05-15', 5, 'Data analyst with expertise in big data.'),\n",
      "(36, 'Lily', 'Mitchell', 'lily.mitchell@company.com', '2023-06-25', 6, 'Marketing specialist with a focus on social media.'),\n",
      "(37, 'Landon', 'Perez', 'landon.perez@company.com', '2023-07-28', 7, 'Sales representative with a focus on consumer sales.'),\n",
      "(38, 'Evelyn', 'Roberts', 'evelyn.roberts@company.com', '2023-08-31', 8, 'Technical support with a focus on hardware issues.'),\n",
      "(39, 'Samuel', 'Turner', 'samuel.turner@company.com', '2023-09-12', 9, 'Finance manager with financial analysis skills.'),\n",
      "(40, 'Grace', 'Phillips', 'grace.phillips@company.com', '2023-10-20', 10, 'Operations manager with project management expertise.'),\n",
      "(41, 'Avery', 'Campbell', 'avery.campbell@company.com', '2023-01-28', 1, 'Software engineer with a focus on security.'),\n",
      "(42, 'Jack', 'Parker', 'jack.parker@company.com', '2023-02-18', 2, 'HR manager with a focus on training and development.'),\n",
      "(43, 'Ella', 'Evans', 'ella.evans@company.com', '2023-03-20', 3, 'Product manager focused on customer satisfaction.'),\n",
      "(44, 'Jackson', 'Edwards', 'jackson.edwards@company.com', '2023-04-30', 4, 'Designer with an eye for detail.'),\n",
      "(45, 'Madison', 'Collins', 'madison.collins@company.com', '2023-05-18', 5, 'Data analyst with a strong research background.'),\n",
      "(46, 'Scarlett', 'Stewart', 'scarlett.stewart@company.com', '2023-06-28', 6, 'Marketing specialist with a focus on market research.'),\n",
      "(47, 'Henry', 'Sanchez', 'henry.sanchez@company.com', '2023-07-30', 7, 'Sales representative with excellent negotiation skills.'),\n",
      "(48, 'Lila', 'Morris', 'lila.morris@company.com', '2023-09-01', 8, 'Technical support with a focus on network issues.'),\n",
      "(49, 'Sebastian', 'Rogers', 'sebastian.rogers@company.com', '2023-09-15', 9, 'Finance manager with strategic planning expertise.'),\n",
      "(50, 'Hannah', 'Reed', 'hannah.reed@company.com', '2023-10-22', 10, 'Operations manager with a focus on process improvement.');\n",
      "\n",
      "INSERT INTO tasks_per_role (role_id, task_description, completion_timeline) VALUES\n",
      "(1, 'Complete Code of Conduct Training', '2023-11-01'),\n",
      "(1, 'Submit Equipment Request Form', '2023-11-05'),\n",
      "(1, 'Setup Development Environment', '2023-11-10'),\n",
      "(1, 'Attend Team Orientation', '2023-11-15'),\n",
      "(1, 'Complete Security Awareness Training', '2023-11-20'),\n",
      "(1, 'Meet with Mentor', '2023-11-22'),\n",
      "(1, 'Review Project Documentation', '2023-11-25'),\n",
      "(1, 'Complete Initial Code Review', '2023-11-30'),\n",
      "(1, 'Sign NDA and Compliance Documents', '2023-12-01'),\n",
      "(1, 'Submit Feedback on Onboarding', '2023-12-05'),\n",
      "(2, 'Complete Code of Conduct Training', '2023-11-01'),\n",
      "(2, 'Submit Equipment Request Form', '2023-11-05'),\n",
      "(2, 'Setup HRIS Account', '2023-11-10'),\n",
      "(2, 'Attend Team Orientation', '2023-11-15'),\n",
      "(2, 'Complete Security Awareness Training', '2023-11-20'),\n",
      "(2, 'Meet with Mentor', '2023-11-22'),\n",
      "(2, 'Review HR Policies', '2023-11-25'),\n",
      "(2, 'Complete Initial HR Review', '2023-11-30'),\n",
      "(2, 'Sign NDA and Compliance Documents', '2023-12-01'),\n",
      "(2, 'Submit Feedback on Onboarding', '2023-12-05'),\n",
      "(3, 'Complete Code of Conduct Training', '2023-11-01'),\n",
      "(3, 'Submit Equipment Request Form', '2023-11-05'),\n",
      "(3, 'Setup Product Management Tools', '2023-11-10'),\n",
      "(3, 'Attend Team Orientation', '2023-11-15'),\n",
      "(3, 'Complete Security Awareness Training', '2023-11-20'),\n",
      "(3, 'Meet with Mentor', '2023-11-22'),\n",
      "(3, 'Review Product Roadmap', '2023-11-25'),\n",
      "(3, 'Complete Initial Product Review', '2023-11-30'),\n",
      "(3, 'Sign NDA and Compliance Documents', '2023-12-01'),\n",
      "(3, 'Submit Feedback on Onboarding', '2023-12-05'),\n",
      "(4, 'Complete Code of Conduct Training', '2023-11-01'),\n",
      "(4, 'Submit Equipment Request Form', '2023-11-05'),\n",
      "(4, 'Setup Design Software', '2023-11-10'),\n",
      "(4, 'Attend Team Orientation', '2023-11-15'),\n",
      "(4, 'Complete Security Awareness Training', '2023-11-20'),\n",
      "(4, 'Meet with Mentor', '2023-11-22'),\n",
      "(4, 'Review Design Guidelines', '2023-11-25'),\n",
      "(4, 'Complete Initial Design Review', '2023-11-30'),\n",
      "(4, 'Sign NDA and Compliance Documents', '2023-12-01'),\n",
      "(4, 'Submit Feedback on Onboarding', '2023-12-05'),\n",
      "(5, 'Complete Code of Conduct Training', '2023-11-01'),\n",
      "(5, 'Submit Equipment Request Form', '2023-11-05'),\n",
      "(5, 'Setup Data Analysis Tools', '2023-11-10'),\n",
      "(5, 'Attend Team Orientation', '2023-11-15'),\n",
      "(5, 'Complete Security Awareness Training', '2023-11-20'),\n",
      "(5, 'Meet with Mentor', '2023-11-22'),\n",
      "(5, 'Review Data Sources', '2023-11-25'),\n",
      "(5, 'Complete Initial Data Analysis', '2023-11-30'),\n",
      "(5, 'Sign NDA and Compliance Documents', '2023-12-01'),\n",
      "(5, 'Submit Feedback on Onboarding', '2023-12-05'),\n",
      "(6, 'Complete Code of Conduct Training', '2023-11-01'),\n",
      "(6, 'Submit Equipment Request Form', '2023-11-05'),\n",
      "(6, 'Setup Marketing Tools', '2023-11-10'),\n",
      "(6, 'Attend Team Orientation', '2023-11-15'),\n",
      "(6, 'Complete Security Awareness Training', '2023-11-20'),\n",
      "(6, 'Meet with Mentor', '2023-11-22'),\n",
      "(6, 'Review Marketing Strategies', '2023-11-25'),\n",
      "(6, 'Complete Initial Campaign Review', '2023-11-30'),\n",
      "(6, 'Sign NDA and Compliance Documents', '2023-12-01'),\n",
      "(6, 'Submit Feedback on Onboarding', '2023-12-05'),\n",
      "(7, 'Complete Code of Conduct Training', '2023-11-01'),\n",
      "(7, 'Submit Equipment Request Form', '2023-11-05'),\n",
      "(7, 'Setup Sales Tools', '2023-11-10'),\n",
      "(7, 'Attend Team Orientation', '2023-11-15'),\n",
      "(7, 'Complete Security Awareness Training', '2023-11-20'),\n",
      "(7, 'Meet with Mentor', '2023-11-22'),\n",
      "(7, 'Review Sales Processes', '2023-11-25'),\n",
      "(7, 'Complete Initial Sales Training', '2023-11-30'),\n",
      "(7, 'Sign NDA and Compliance Documents', '2023-12-01'),\n",
      "(7, 'Submit Feedback on Onboarding', '2023-12-05'),\n",
      "(8, 'Complete Code of Conduct Training', '2023-11-01'),\n",
      "(8, 'Submit Equipment Request Form', '2023-11-05'),\n",
      "(8, 'Setup Support Systems', '2023-11-10'),\n",
      "(8, 'Attend Team Orientation', '2023-11-15'),\n",
      "(8, 'Complete Security Awareness Training', '2023-11-20'),\n",
      "(8, 'Meet with Mentor', '2023-11-22'),\n",
      "(8, 'Review Support Protocols', '2023-11-25'),\n",
      "(8, 'Complete Initial Support Training', '2023-11-30'),\n",
      "(8, 'Sign NDA and Compliance Documents', '2023-12-01'),\n",
      "(8, 'Submit Feedback on Onboarding', '2023-12-05'),\n",
      "(9, 'Complete Code of Conduct Training', '2023-11-01'),\n",
      "(9, 'Submit Equipment Request Form', '2023-11-05'),\n",
      "(9, 'Setup Finance Systems', '2023-11-10'),\n",
      "(9, 'Attend Team Orientation', '2023-11-15'),\n",
      "(9, 'Complete Security Awareness Training', '2023-11-20'),\n",
      "(9, 'Meet with Mentor', '2023-11-22'),\n",
      "(9, 'Review Financial Guidelines', '2023-11-25'),\n",
      "(9, 'Complete Initial Finance Review', '2023-11-30'),\n",
      "(9, 'Sign NDA and Compliance Documents', '2023-12-01'),\n",
      "(9, 'Submit Feedback on Onboarding', '2023-12-05'),\n",
      "(10, 'Complete Code of Conduct Training', '2023-11-01'),\n",
      "(10, 'Submit Equipment Request Form', '2023-11-05'),\n",
      "(10, 'Setup Operations Systems', '2023-11-10'),\n",
      "(10, 'Attend Team Orientation', '2023-11-15'),\n",
      "(10, 'Complete Security Awareness Training', '2023-11-20'),\n",
      "(10, 'Meet with Mentor', '2023-11-22'),\n",
      "(10, 'Review Operational Procedures', '2023-11-25'),\n",
      "(10, 'Complete Initial Operations Review', '2023-11-30'),\n",
      "(10, 'Sign NDA and Compliance Documents', '2023-12-01'),\n",
      "(10, 'Submit Feedback on Onboarding', '2023-12-05');\n"
     ]
    }
   ],
   "source": [
    "# TODO: Write a prompt to generate realistic seed data.\n",
    "\n",
    "\n",
    "cleaned_schema = load_artifact(\"artifacts/waffle_schema.sql\")\n",
    "seed_data_prompt = f\"\"\"\n",
    "You are a Database Analyst specializing in test data generation. Your task is to create realistic seed data that is STRICTLY COMPLIANT with the provided database schema.\n",
    "\n",
    "**Critical Requirements:**\n",
    "- Generate atleast 50 INSERT statements for EACH table defined in the schema (user_roles,users)\n",
    "- Generate 10 training related documents and 10 document requirements related to each of the 10 roles in tasks per role table\n",
    "- ALL data must conform to the exact data types specified in the schema (VARCHAR lengths, INTEGER ranges, DATE formats, etc.)\n",
    "- STRICTLY respect all constraints defined in the schema:\n",
    "  * PRIMARY KEY constraints (ensure uniqueness)\n",
    "  * FOREIGN KEY constraints (ensure referential integrity - all FK values must reference existing PK values)\n",
    "  * NOT NULL constraints (provide values for ALL NOT NULL columns)\n",
    "  * UNIQUE constraints (ensure no duplicates)\n",
    "  * CHECK constraints (satisfy all validation rules)\n",
    "  * DEFAULT values (you may use them or override with explicit values)\n",
    "- Maintain proper insert order: parent tables BEFORE child tables to satisfy FK dependencies\n",
    "- Use realistic, contextually appropriate data for a new hire onboarding tool\n",
    "\n",
    "**Data Guidelines:**\n",
    "- User names: realistic full names (e.g., \"Sarah Johnson\", \"Michael Chen\")\n",
    "- Task titles: relevant onboarding activities (e.g., \"Complete I-9 Form\", \"Setup Workstation\", \"Attend Security Training\")\n",
    "- Dates: use logical sequences (hire dates before task due dates, etc.)\n",
    "- Email addresses: follow standard format (firstname.lastname@company.com)\n",
    "- Status values: match any ENUM or CHECK constraint definitions exactly\n",
    "- IDs: start from 1 and increment sequentially\n",
    "\n",
    "**Output Format:**\n",
    "Provide ONLY executable SQL INSERT statements:\n",
    "- No markdown formatting or code blocks\n",
    "- No explanatory comments or documentation\n",
    "- One INSERT statement per line or proper multi-row INSERT syntax\n",
    "- Statements must be ready to execute immediately after CREATE TABLE statements\n",
    "\n",
    "**Context:**\n",
    "PRD: {prd_content}\n",
    "Schema: {cleaned_schema}\n",
    "\n",
    "**Validation Checklist (verify before output):**\n",
    "□ All FK values reference existing PK values\n",
    "□ All NOT NULL columns have values\n",
    "□ All data types match schema exactly\n",
    "□ All string lengths within VARCHAR limits\n",
    "□ Proper insert order maintained\n",
    "□ All constraints satisfied\n",
    "\"\"\"\n",
    "#enhanced_seed_data_prompt = prompt_enhancer(seed_data_prompt)\n",
    "\n",
    "print(\"--- Generating Seed Data ---\")\n",
    "if prd_content and cleaned_schema:\n",
    "    generated_seed_data = get_completion(seed_data_prompt, client, model_name, api_provider)\n",
    "    \n",
    "    # Clean up the generated seed data\n",
    "    cleaned_seed_data = clean_llm_output(generated_seed_data, language='sql')\n",
    "    print(cleaned_seed_data)\n",
    "    \n",
    "    # Save the cleaned seed data\n",
    "    save_artifact(cleaned_seed_data, 'artifacts/waffle_seed_data.sql',overwrite=True)\n",
    "else:\n",
    "    print(\"Skipping seed data generation because PRD or schema is missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3 (Advanced): Creating and Seeding a Live Database\n",
    "\n",
    "**Task:** This is a critical technical step. You will write a Python script to execute the generated SQL files, creating a live `onboarding.db` file that your application will use.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Complete the `create_database` function below.\n",
    "2.  The function should first connect to (and thus create) a SQLite database file named `artifacts/onboarding.db`.\n",
    "3.  It should then open and execute the `schema.sql` file to create the tables.\n",
    "4.  Finally, it should open and execute the `seed_data.sql` file to populate the tables.\n",
    "5.  Use a `try...finally` block to ensure the database connection is always closed, even if an error occurs.\n",
    "\n",
    "> **Hint:** The `try...finally` block is a crucial Python pattern. The code in the `finally` block will run whether the `try` block succeeds or fails, making it the perfect place to ensure resources like database connections are always closed.\n",
    "\n",
    "**Expected Quality:** A physical `onboarding.db` file in your `artifacts` folder. This is a tangible asset that proves your design is valid and provides a concrete foundation for backend development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def tables_to_drop(schema_path):\n",
    "    \"\"\"Extracts all table names from CREATE TABLE statements in the schema file.\"\"\"\n",
    "    if not os.path.exists(schema_path):\n",
    "        print(f\"Error: Schema file not found at {schema_path}\")\n",
    "        return []\n",
    "    with open(schema_path, 'r', encoding='utf-8') as f:\n",
    "        sql = f.read()\n",
    "    # Regex to match CREATE TABLE [IF NOT EXISTS] table_name\n",
    "    pattern = r\"CREATE TABLE(?: IF NOT EXISTS)?\\s+([a-zA-Z0-9_]+)\"\n",
    "    tables = re.findall(pattern, sql, re.IGNORECASE)\n",
    "    return tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to database at c:\\Users\\labadmin\\Desktop\\Repository\\AG-AISOFTDEV\\artifacts\\waffle_tech_suite.db\n",
      "Checking for existing tables in database...\n",
      "Found 3 existing tables: ['user_roles', 'users', 'tasks_per_role']\n",
      "  Dropped table: user_roles\n",
      "  Dropped table: users\n",
      "  Dropped table: tasks_per_role\n",
      "All existing tables dropped successfully.\n",
      "Schema-based table cleanup completed.\n",
      "Tables created successfully.\n",
      "Seed data inserted successfully.\n",
      "Database changes committed.\n",
      "Database connection closed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def create_database(db_path, schema_path, seed_path):\n",
    "    \"\"\"Creates and seeds a SQLite database from SQL files.\"\"\"\n",
    "    if not os.path.exists(schema_path):\n",
    "        print(f\"Error: Schema file not found at {schema_path}\")\n",
    "        return\n",
    "    \n",
    "    conn = None\n",
    "    try:\n",
    "        # Connect to the SQLite database. This will create the file if it doesn't exist.\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        cursor = conn.cursor()\n",
    "        print(f\"Successfully connected to database at {db_path}\")\n",
    "\n",
    "        # --- Drop ALL existing tables in the database ---\n",
    "        print(\"Checking for existing tables in database...\")\n",
    "        cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%';\")\n",
    "        existing_tables = cursor.fetchall()\n",
    "        \n",
    "        if existing_tables:\n",
    "            print(f\"Found {len(existing_tables)} existing tables: {[table[0] for table in existing_tables]}\")\n",
    "            # Disable foreign key constraints temporarily to allow dropping tables\n",
    "            cursor.execute(\"PRAGMA foreign_keys = OFF;\")\n",
    "            \n",
    "            for table in existing_tables:\n",
    "                table_name = table[0]\n",
    "                cursor.execute(f\"DROP TABLE IF EXISTS {table_name};\")\n",
    "                print(f\"  Dropped table: {table_name}\")\n",
    "            \n",
    "            # Re-enable foreign key constraints\n",
    "            cursor.execute(\"PRAGMA foreign_keys = ON;\")\n",
    "            print(\"All existing tables dropped successfully.\")\n",
    "        else:\n",
    "            print(\"No existing tables found in database.\")\n",
    "\n",
    "        # --- Additional safety: Drop tables from schema.sql if any remain ---\n",
    "        tables_from_schema = tables_to_drop(schema_path)\n",
    "        for table in tables_from_schema:\n",
    "            cursor.execute(f\"DROP TABLE IF EXISTS {table};\")\n",
    "        print(\"Schema-based table cleanup completed.\")\n",
    "\n",
    "        # Read the content of the schema file using load_artifact.\n",
    "        schema_sql = load_artifact(schema_path)\n",
    "        \n",
    "        # Replace TIMESTAMP WITH TIME ZONE with TIMESTAMP for SQLite compatibility\n",
    "        if schema_sql:\n",
    "            schema_sql = schema_sql.replace(\"TIMESTAMP WITH TIME ZONE\", \"TIMESTAMP\")\n",
    "            cursor.executescript(schema_sql)\n",
    "            print(\"Tables created successfully.\")\n",
    "\n",
    "        # Check if the seed data file exists. If it does, load and execute it.\n",
    "        if os.path.exists(seed_path):\n",
    "            seed_sql = load_artifact(seed_path)\n",
    "            if seed_sql:\n",
    "                cursor.executescript(seed_sql)\n",
    "                print(\"Seed data inserted successfully.\")\n",
    "\n",
    "        # Commit the changes to the database.\n",
    "        conn.commit()\n",
    "        print(\"Database changes committed.\")    \n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Database error: {e}\")\n",
    "    finally:\n",
    "        # Ensure the connection is closed if it was opened.\n",
    "        if conn:\n",
    "            conn.close()\n",
    "            print(\"Database connection closed.\")\n",
    "\n",
    "def query_table(db_path, query, params=None):\n",
    "    if not os.path.exists(db_path):\n",
    "        print(f\"Error: Database file not found at {db_path}\")\n",
    "        return None\n",
    "    conn = None\n",
    "    try:\n",
    "        # Connect to the SQLite database\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        cursor = conn.cursor()\n",
    "        # Execute the query with or without parameters\n",
    "        if params:\n",
    "            cursor.execute(query, params)\n",
    "        else:\n",
    "            cursor.execute(query)\n",
    "        # Fetch all results\n",
    "        results = cursor.fetchall()\n",
    "        # Get column names for better readability (optional)\n",
    "        column_names = [description[0] for description in cursor.description]\n",
    "        print(f\"Query executed successfully. Found {len(results)} rows.\")\n",
    "        print(f\"Columns: {', '.join(column_names)}\")\n",
    "        return results\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Database query error: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        # Ensure the connection is closed if it was opened\n",
    "        if conn:\n",
    "            conn.close()\n",
    "try:\n",
    "    project_root\n",
    "except NameError:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "\n",
    "db_file = os.path.join(project_root, \"artifacts\", \"waffle_tech_suite.db\")\n",
    "schema_file = os.path.join(project_root, \"artifacts\", \"waffle_schema.sql\")\n",
    "seed_file = os.path.join(project_root, \"artifacts\", \"waffle_seed_data.sql\")\n",
    "\n",
    "# Execute the function\n",
    "create_database(db_file, schema_file, seed_file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the database was created and populated correctly\n",
    "print(\"=== Database Verification ===\")\n",
    "\n",
    "# Query user_roles table\n",
    "print(\"\\n--- User Roles ---\")\n",
    "roles_results = query_table(db_file, \"SELECT * FROM user_roles ORDER BY role_id\")\n",
    "if roles_results:\n",
    "    for row in roles_results:\n",
    "        print(f\"Role ID: {row[0]}, Role Name: {row[1]}\")\n",
    "\n",
    "# Query users table\n",
    "print(\"\\n--- Users ---\")\n",
    "users_results = query_table(db_file, \"SELECT * FROM users ORDER BY user_id\")\n",
    "if users_results:\n",
    "    for row in users_results:\n",
    "        print(f\"User ID: {row[0]}, Name: {row[1]} {row[2]}, Email: {row[3]}, Role ID: {row[5]}\")\n",
    "\n",
    "# Query with JOIN to show user roles\n",
    "print(\"\\n--- Users with Role Names ---\")\n",
    "join_results = query_table(db_file, \"\"\"\n",
    "    SELECT u.user_id, u.first_name, u.last_name, u.email, ur.role_name \n",
    "    FROM users u \n",
    "    JOIN user_roles ur ON u.role_id = ur.role_id \n",
    "    ORDER BY u.user_id\n",
    "\"\"\")\n",
    "if join_results:\n",
    "    for row in join_results:\n",
    "        print(f\"User: {row[1]} {row[2]} ({row[3]}) - Role: {row[4]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final status check\n",
    "import os\n",
    "\n",
    "print(\"=== Lab Completion Status ===\")\n",
    "print(f\"✅ Database file created: {os.path.exists(db_file)}\")\n",
    "print(f\"✅ Schema file exists: {os.path.exists(schema_file)}\")\n",
    "print(f\"✅ Seed data file exists: {os.path.exists(seed_file)}\")\n",
    "print(f\"✅ Database contains {len(users_results)} users\")\n",
    "print(f\"✅ Database contains {len(roles_results)} user roles\")\n",
    "print(f\"\\n🎉 Lab completed successfully!\")\n",
    "print(f\"Database location: {db_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write a prompt to generate the SQL schema from the PRD.\n",
    "additional_table_prompt = f\"\"\"\n",
    "You are a Senior Database Architect. Design a comprehensive, production-ready SQL database schema.\n",
    "I would like to add a new table to the existing schema {cleaned_schema} for displaying pre-defined onboarding tasks per user roles and their completion timeline. \n",
    "\n",
    "\n",
    "**Requirements:**\n",
    "- Create 1 table only (e.g., tasks_per_role)\n",
    "- This should be related to user_roles table via foreign key\n",
    "- Follow database normalization best practices \n",
    "- Use standard SQL naming conventions (lowercase with underscores)\n",
    "- Consider data integrity, scalability, and query performance\n",
    "- Use AUTO_INCREMENT for primary keys; instead\n",
    "- Use SQL Lite compatible syntax only\n",
    "\n",
    "**Output Format:**\n",
    "Provide ONLY the raw SQL CREATE TABLE statement for the new table (CREATE 1 TABLE ONLY) without:\n",
    "- Markdown code blocks or formatting\n",
    "- Explanatory text or comments\n",
    "- Additional documentation\n",
    "\n",
    " CREATE TABLE statement should be complete and executable. Remove any duplicate table definitions if any.\n",
    "**Existing schema:**\n",
    "{cleaned_schema}\n",
    "\"\"\"\n",
    "\n",
    "#enhanced_schema_prompt = prompt_enhancer(schema_prompt)\n",
    "print(\"--- Generating SQL Schema ---\")\n",
    "if prd_content:\n",
    "    additional_table = get_completion(additional_table_prompt, client, model_name, api_provider)\n",
    "    print(additional_table)\n",
    "\n",
    "    # Clean up the generated schema using our helper function\n",
    "    cleaned_table_schema = clean_llm_output(additional_table, language='sql')\n",
    "    print(cleaned_table_schema)\n",
    "\n",
    "    # Save the cleaned schema\n",
    "    save_artifact(cleaned_table_schema, 'artifacts/waffle_final_schema.sql',overwrite=True)\n",
    "else:\n",
    "    print(\"Skipping schema generation because PRD is missing.\")\n",
    "    cleaned_table_schema = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Conclusion\n",
    "\n",
    "Excellent work! You have now moved from abstract requirements to a concrete, physical database artifact. You've used an LLM to design a schema, generate realistic test data, and then used a Python script to bring that database to life. This `onboarding.db` file is the foundation upon which we will build our API in Day 3.\n",
    "\n",
    "> **Key Takeaway:** The ability to generate structured data definitions (like a SQL schema) from unstructured text (like a PRD) is a core skill in AI-assisted development. It automates a critical and often time-consuming design step."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
