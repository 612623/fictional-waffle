{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2 - Lab 1: AI-Generated System Design & Database Seeding\n",
    "\n",
    "**Objective:** Use the PRD artifact from Day 1 to generate a detailed SQL database schema, create realistic seed data, and then use those outputs to create and seed a live, local database file.\n",
    "\n",
    "**Estimated Time:** 150 minutes\n",
    "\n",
    "**Introduction:**\n",
    "Welcome to Day 2! Today, we transition from *what* we're building to *how* we'll build it. In this lab, you will act as the lead architect for the Onboarding Tool. Your task is to use the PRD to define the data structure of the application and create a tangible database artifact that will be used for the rest of the course.\n",
    "\n",
    "For definitions of key terms used in this lab, please refer to the [GLOSSARY.md](../../GLOSSARY.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "We will load the `day1_prd.md` artifact from Day 1. This document is the primary source of truth for our project and provides the necessary context for the LLM to make intelligent design suggestions.\n",
    "\n",
    "**Model Selection:**\n",
    "Feel free to experiment with different models by changing the `model_name` in `setup_llm_client()`. Models with strong reasoning capabilities, like `gpt-4o`, `o3`, or `gemini-2.5-pro`, are excellent choices for design tasks.\n",
    "\n",
    "**Helper Functions Used:**\n",
    "- `setup_llm_client()`: To configure the API client.\n",
    "- `get_completion()`: To send prompts to the LLM.\n",
    "- `load_artifact()`: To read the PRD from the `artifacts` directory.\n",
    "- `save_artifact()`: To save the generated SQL schema and seed data.\n",
    "- `clean_llm_output()`: To remove markdown fences from the generated SQL code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-02 14:07:19,608 ag_aisoftdev.utils INFO LLM Client configured provider=openai model=gpt-4o latency_ms=None artifacts_path=None\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import sqlite3\n",
    "\n",
    "# Add the project's root directory to the Python path to ensure 'utils' can be imported.\n",
    "try:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "except IndexError:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from utils import setup_llm_client, get_completion, save_artifact, load_artifact, clean_llm_output,prompt_enhancer\n",
    "\n",
    "client, model_name, api_provider = setup_llm_client(model_name=\"gpt-4o\")\n",
    "\n",
    "# Load the PRD from Day 1\n",
    "prd_content = load_artifact(\"artifacts/waffle_PRD_output.md\")\n",
    "\n",
    "if not prd_content:\n",
    "    print(\"Warning: Could not load waffle_PRD_output.md. Lab may not function correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: The Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1 (Foundational): Generating the SQL Schema\n",
    "\n",
    "**Task:** Use the PRD to generate a normalized SQL schema for the application.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Create a prompt that instructs the LLM to act as a Database Administrator (DBA).\n",
    "2.  Provide the `prd_content` as context.\n",
    "3.  Ask the LLM to design a normalized SQL schema with at least two tables (e.g., `users` and `onboarding_tasks`).\n",
    "4.  The output should be the raw `CREATE TABLE` statements.\n",
    "5.  Save the generated SQL to `artifacts/schema.sql`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating SQL Schema ---\n",
      "CREATE TABLE users (\n",
      "    user_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
      "    first_name TEXT NOT NULL,\n",
      "    last_name TEXT NOT NULL,\n",
      "    email TEXT NOT NULL UNIQUE,\n",
      "    password_hash TEXT NOT NULL,\n",
      "    role_id INTEGER NOT NULL,\n",
      "    start_date DATE NOT NULL,\n",
      "    FOREIGN KEY (role_id) REFERENCES user_roles(role_id)\n",
      ");\n",
      "\n",
      "CREATE TABLE user_roles (\n",
      "    role_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
      "    role_name TEXT NOT NULL UNIQUE\n",
      ");\n",
      "CREATE TABLE users (\n",
      "    user_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
      "    first_name TEXT NOT NULL,\n",
      "    last_name TEXT NOT NULL,\n",
      "    email TEXT NOT NULL UNIQUE,\n",
      "    password_hash TEXT NOT NULL,\n",
      "    role_id INTEGER NOT NULL,\n",
      "    start_date DATE NOT NULL,\n",
      "    FOREIGN KEY (role_id) REFERENCES user_roles(role_id)\n",
      ");\n",
      "\n",
      "CREATE TABLE user_roles (\n",
      "    role_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
      "    role_name TEXT NOT NULL UNIQUE\n",
      ");\n",
      "CREATE TABLE users (\n",
      "    user_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
      "    first_name TEXT NOT NULL,\n",
      "    last_name TEXT NOT NULL,\n",
      "    email TEXT NOT NULL UNIQUE,\n",
      "    password_hash TEXT NOT NULL,\n",
      "    role_id INTEGER NOT NULL,\n",
      "    start_date DATE NOT NULL,\n",
      "    FOREIGN KEY (role_id) REFERENCES user_roles(role_id)\n",
      ");\n",
      "\n",
      "CREATE TABLE user_roles (\n",
      "    role_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
      "    role_name TEXT NOT NULL UNIQUE\n",
      ");\n",
      "CREATE TABLE users (\n",
      "    user_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
      "    first_name TEXT NOT NULL,\n",
      "    last_name TEXT NOT NULL,\n",
      "    email TEXT NOT NULL UNIQUE,\n",
      "    password_hash TEXT NOT NULL,\n",
      "    role_id INTEGER NOT NULL,\n",
      "    start_date DATE NOT NULL,\n",
      "    FOREIGN KEY (role_id) REFERENCES user_roles(role_id)\n",
      ");\n",
      "\n",
      "CREATE TABLE user_roles (\n",
      "    role_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
      "    role_name TEXT NOT NULL UNIQUE\n",
      ");\n"
     ]
    }
   ],
   "source": [
    "# TODO: Write a prompt to generate the SQL schema from the PRD.\n",
    "schema_prompt = f\"\"\"\n",
    "You are a Senior Database Architect. Design a comprehensive, production-ready SQL database schema based on the Product Requirements Document (PRD) provided below.\n",
    "\n",
    "**Requirements:**\n",
    "- Create not more than 2 tables (e.g., users and user_roles)\n",
    "- Include all necessary tables to fully support the application's features and workflows\n",
    "- Define appropriate columns with proper data types for each table\n",
    "- Establish primary keys for all tables\n",
    "- Define foreign keys and relationships between tables where applicable\n",
    "- Follow database normalization best practices (at least 3NF)\n",
    "- Use standard SQL naming conventions (lowercase with underscores)\n",
    "- Include appropriate indexes for performance optimization\n",
    "- Add constraints (NOT NULL, UNIQUE, CHECK) where logically necessary\n",
    "- Consider data integrity, scalability, and query performance\n",
    "- Use AUTO_INCREMENT for primary keys; instead\n",
    "- Use SQL Lite compatible syntax only\n",
    "\n",
    "**Output Format:**\n",
    "Provide ONLY the raw SQL CREATE TABLE statements without:\n",
    "- Markdown code blocks or formatting\n",
    "- Explanatory text or comments\n",
    "- Additional documentation\n",
    "\n",
    "Each CREATE TABLE statement should be complete and executable.\n",
    "\n",
    "**PRD:**\n",
    "{prd_content}\n",
    "\"\"\"\n",
    "\n",
    "#enhanced_schema_prompt = prompt_enhancer(schema_prompt)\n",
    "print(\"--- Generating SQL Schema ---\")\n",
    "if prd_content:\n",
    "    generated_schema = get_completion(schema_prompt, client, model_name, api_provider)\n",
    "    print(generated_schema)\n",
    "    \n",
    "    # Clean up the generated schema using our helper function\n",
    "    cleaned_schema = clean_llm_output(generated_schema, language='sql')\n",
    "    print(cleaned_schema)\n",
    "    \n",
    "    # Save the cleaned schema\n",
    "    save_artifact(cleaned_schema, 'artifacts/waffle_schema.sql',overwrite=True)\n",
    "else:\n",
    "    print(\"Skipping schema generation because PRD is missing.\")\n",
    "    cleaned_schema = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2 (Intermediate): Generating Realistic Seed Data\n",
    "\n",
    "**Task:** Prompt the LLM to generate realistic seed data that conforms to the schema you just created.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Create a new prompt.\n",
    "2.  Provide both the `prd_content` and the `cleaned_schema` as context.\n",
    "3.  Instruct the LLM to generate 5-10 realistic `INSERT` statements for your tables.\n",
    "4.  The data should be relevant to a new hire onboarding tool (e.g., sample user names and task titles like \"Complete HR Paperwork\").\n",
    "5.  Save the generated `INSERT` statements to `artifacts/seed_data.sql`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating Seed Data ---\n",
      "INSERT INTO user_roles (role_id, role_name) VALUES (1, 'Software Engineer');\n",
      "INSERT INTO user_roles (role_id, role_name) VALUES (2, 'HR Manager');\n",
      "INSERT INTO user_roles (role_id, role_name) VALUES (3, 'Project Manager');\n",
      "INSERT INTO user_roles (role_id, role_name) VALUES (4, 'Product Owner');\n",
      "INSERT INTO user_roles (role_id, role_name) VALUES (5, 'Data Analyst');\n",
      "INSERT INTO user_roles (role_id, role_name) VALUES (6, 'Designer');\n",
      "INSERT INTO user_roles (role_id, role_name) VALUES (7, 'QA Engineer');\n",
      "INSERT INTO user_roles (role_id, role_name) VALUES (8, 'Marketing Specialist');\n",
      "INSERT INTO user_roles (role_id, role_name) VALUES (9, 'Sales Representative');\n",
      "INSERT INTO user_roles (role_id, role_name) VALUES (10, 'Customer Support');\n",
      "\n",
      "INSERT INTO users (user_id, first_name, last_name, email, hire_date, role_id, bio) VALUES (1, 'John', 'Doe', 'john.doe@company.com', '2023-01-02', 1, 'Experienced software engineer with a passion for developing innovative programs.');\n",
      "INSERT INTO users (user_id, first_name, last_name, email, hire_date, role_id, bio) VALUES (2, 'Jane', 'Smith', 'jane.smith@company.com', '2023-01-03', 2, 'HR manager with a knack for improving employee relations and fostering a vibrant workplace culture.');\n",
      "INSERT INTO users (user_id, first_name, last_name, email, hire_date, role_id, bio) VALUES (3, 'Alice', 'Brown', 'alice.brown@company.com', '2023-01-04', 3, 'Project manager who excels at orchestrating complex project requirements and tasks.');\n",
      "INSERT INTO users (user_id, first_name, last_name, email, hire_date, role_id, bio) VALUES (4, 'Robert', 'Johnson', 'robert.johnson@company.com', '2023-01-05', 4, 'Product owner committed to delivering user-centric products.');\n",
      "INSERT INTO users (user_id, first_name, last_name, email, hire_date, role_id, bio) VALUES (5, 'Emily', 'Davis', 'emily.davis@company.com', '2023-01-06', 5, 'Data analyst with a keen eye for detail and a passion for data-driven insights.');\n",
      "INSERT INTO users (user_id, first_name, last_name, email, hire_date, role_id, bio) VALUES (6, 'Michael', 'Wilson', 'michael.wilson@company.com', '2023-01-07', 6, 'Creative designer with a flair for innovative visual solutions.');\n",
      "INSERT INTO users (user_id, first_name, last_name, email, hire_date, role_id, bio) VALUES (7, 'Sarah', 'Moore', 'sarah.moore@company.com', '2023-01-08', 7, 'QA engineer dedicated to ensuring product quality and reliability.');\n",
      "INSERT INTO users (user_id, first_name, last_name, email, hire_date, role_id, bio) VALUES (8, 'David', 'Taylor', 'david.taylor@company.com', '2023-01-09', 8, 'Marketing specialist focused on building brand awareness and driving sales.');\n",
      "INSERT INTO users (user_id, first_name, last_name, email, hire_date, role_id, bio) VALUES (9, 'Laura', 'Anderson', 'laura.anderson@company.com', '2023-01-10', 9, 'Sales representative skilled in closing deals and building customer relationships.');\n",
      "INSERT INTO users (user_id, first_name, last_name, email, hire_date, role_id, bio) VALUES (10, 'James', 'Thomas', 'james.thomas@company.com', '2023-01-11', 10, 'Customer support specialist committed to resolving customer issues efficiently.');\n",
      "\n",
      "-- Additional 40 users to follow the pattern (ensure unique emails and hire dates)\n",
      "\n",
      "INSERT INTO tasks_per_role (role_id, task_description, completion_timeline) VALUES (1, 'Complete I-9 Form', '2023-02-01');\n",
      "INSERT INTO tasks_per_role (role_id, task_description, completion_timeline) VALUES (1, 'Setup Workstation', '2023-02-02');\n",
      "INSERT INTO tasks_per_role (role_id, task_description, completion_timeline) VALUES (1, 'Attend Security Training', '2023-02-03');\n",
      "INSERT INTO tasks_per_role (role_id, task_description, completion_timeline) VALUES (1, 'Submit Code of Conduct Acknowledgment', '2023-02-04');\n",
      "INSERT INTO tasks_per_role (role_id, task_description, completion_timeline) VALUES (1, 'Complete Benefits Enrollment', '2023-02-05');\n",
      "INSERT INTO tasks_per_role (role_id, task_description, completion_timeline) VALUES (2, 'Review Company Policies', '2023-02-01');\n",
      "INSERT INTO tasks_per_role (role_id, task_description, completion_timeline) VALUES (2, 'Conduct New Hire Orientation', '2023-02-02');\n",
      "INSERT INTO tasks_per_role (role_id, task_description, completion_timeline) VALUES (2, 'Set Up Employee Files', '2023-02-03');\n",
      "INSERT INTO tasks_per_role (role_id, task_description, completion_timeline) VALUES (2, 'Monitor Compliance Training', '2023-02-04');\n",
      "INSERT INTO tasks_per_role (role_id, task_description, completion_timeline) VALUES (2, 'Facilitate Team Introductions', '2023-02-05');\n",
      "\n",
      "-- Additional tasks for other roles following the pattern, ensuring unique task descriptions and timelines\n"
     ]
    }
   ],
   "source": [
    "# TODO: Write a prompt to generate realistic seed data.\n",
    "\n",
    "\n",
    "cleaned_schema = load_artifact(\"artifacts/waffle_schema.sql\")\n",
    "seed_data_prompt = f\"\"\"\n",
    "You are a Database Analyst specializing in test data generation. Your task is to create realistic seed data that is STRICTLY COMPLIANT with the provided database schema.\n",
    "\n",
    "**Critical Requirements:**\n",
    "- Generate atleast 50 INSERT statements for EACH table defined in the schema (user_roles,tasks_per_role,users)\n",
    "- ALL data must conform to the exact data types specified in the schema (VARCHAR lengths, INTEGER ranges, DATE formats, etc.)\n",
    "- STRICTLY respect all constraints defined in the schema:\n",
    "  * PRIMARY KEY constraints (ensure uniqueness)\n",
    "  * FOREIGN KEY constraints (ensure referential integrity - all FK values must reference existing PK values)\n",
    "  * NOT NULL constraints (provide values for ALL NOT NULL columns)\n",
    "  * UNIQUE constraints (ensure no duplicates)\n",
    "  * CHECK constraints (satisfy all validation rules)\n",
    "  * DEFAULT values (you may use them or override with explicit values)\n",
    "- Maintain proper insert order: parent tables BEFORE child tables to satisfy FK dependencies\n",
    "- Use realistic, contextually appropriate data for a new hire onboarding tool\n",
    "\n",
    "**Data Guidelines:**\n",
    "- User names: realistic full names (e.g., \"Sarah Johnson\", \"Michael Chen\")\n",
    "- Task titles: relevant onboarding activities (e.g., \"Complete I-9 Form\", \"Setup Workstation\", \"Attend Security Training\")\n",
    "- Dates: use logical sequences (hire dates before task due dates, etc.)\n",
    "- Email addresses: follow standard format (firstname.lastname@company.com)\n",
    "- Status values: match any ENUM or CHECK constraint definitions exactly\n",
    "- IDs: start from 1 and increment sequentially\n",
    "\n",
    "**Output Format:**\n",
    "Provide ONLY executable SQL INSERT statements:\n",
    "- No markdown formatting or code blocks\n",
    "- No explanatory comments or documentation\n",
    "- One INSERT statement per line or proper multi-row INSERT syntax\n",
    "- Statements must be ready to execute immediately after CREATE TABLE statements\n",
    "\n",
    "**Context:**\n",
    "PRD: {prd_content}\n",
    "Schema: {cleaned_schema}\n",
    "\n",
    "**Validation Checklist (verify before output):**\n",
    "□ All FK values reference existing PK values\n",
    "□ All NOT NULL columns have values\n",
    "□ All data types match schema exactly\n",
    "□ All string lengths within VARCHAR limits\n",
    "□ Proper insert order maintained\n",
    "□ All constraints satisfied\n",
    "\"\"\"\n",
    "#enhanced_seed_data_prompt = prompt_enhancer(seed_data_prompt)\n",
    "\n",
    "print(\"--- Generating Seed Data ---\")\n",
    "if prd_content and cleaned_schema:\n",
    "    generated_seed_data = get_completion(seed_data_prompt, client, model_name, api_provider)\n",
    "    \n",
    "    # Clean up the generated seed data\n",
    "    cleaned_seed_data = clean_llm_output(generated_seed_data, language='sql')\n",
    "    print(cleaned_seed_data)\n",
    "    \n",
    "    # Save the cleaned seed data\n",
    "    save_artifact(cleaned_seed_data, 'artifacts/waffle_seed_data.sql',overwrite=True)\n",
    "else:\n",
    "    print(\"Skipping seed data generation because PRD or schema is missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3 (Advanced): Creating and Seeding a Live Database\n",
    "\n",
    "**Task:** This is a critical technical step. You will write a Python script to execute the generated SQL files, creating a live `onboarding.db` file that your application will use.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Complete the `create_database` function below.\n",
    "2.  The function should first connect to (and thus create) a SQLite database file named `artifacts/onboarding.db`.\n",
    "3.  It should then open and execute the `schema.sql` file to create the tables.\n",
    "4.  Finally, it should open and execute the `seed_data.sql` file to populate the tables.\n",
    "5.  Use a `try...finally` block to ensure the database connection is always closed, even if an error occurs.\n",
    "\n",
    "> **Hint:** The `try...finally` block is a crucial Python pattern. The code in the `finally` block will run whether the `try` block succeeds or fails, making it the perfect place to ensure resources like database connections are always closed.\n",
    "\n",
    "**Expected Quality:** A physical `onboarding.db` file in your `artifacts` folder. This is a tangible asset that proves your design is valid and provides a concrete foundation for backend development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def tables_to_drop(schema_path):\n",
    "    \"\"\"Extracts all table names from CREATE TABLE statements in the schema file.\"\"\"\n",
    "    if not os.path.exists(schema_path):\n",
    "        print(f\"Error: Schema file not found at {schema_path}\")\n",
    "        return []\n",
    "    with open(schema_path, 'r', encoding='utf-8') as f:\n",
    "        sql = f.read()\n",
    "    # Regex to match CREATE TABLE [IF NOT EXISTS] table_name\n",
    "    pattern = r\"CREATE TABLE(?: IF NOT EXISTS)?\\s+([a-zA-Z0-9_]+)\"\n",
    "    tables = re.findall(pattern, sql, re.IGNORECASE)\n",
    "    return tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to database at c:\\Users\\labadmin\\Desktop\\Repository\\AG-AISOFTDEV\\artifacts\\waffle_tech_suite.db\n",
      "Checking for existing tables in database...\n",
      "Found 3 existing tables: ['user_roles', 'users', 'tasks_per_role']\n",
      "  Dropped table: user_roles\n",
      "  Dropped table: users\n",
      "  Dropped table: tasks_per_role\n",
      "All existing tables dropped successfully.\n",
      "Schema-based table cleanup completed.\n",
      "Tables created successfully.\n",
      "Seed data inserted successfully.\n",
      "Database changes committed.\n",
      "Database connection closed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def create_database(db_path, schema_path, seed_path):\n",
    "    \"\"\"Creates and seeds a SQLite database from SQL files.\"\"\"\n",
    "    if not os.path.exists(schema_path):\n",
    "        print(f\"Error: Schema file not found at {schema_path}\")\n",
    "        return\n",
    "    \n",
    "    conn = None\n",
    "    try:\n",
    "        # Connect to the SQLite database. This will create the file if it doesn't exist.\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        cursor = conn.cursor()\n",
    "        print(f\"Successfully connected to database at {db_path}\")\n",
    "\n",
    "        # --- Drop ALL existing tables in the database ---\n",
    "        print(\"Checking for existing tables in database...\")\n",
    "        cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%';\")\n",
    "        existing_tables = cursor.fetchall()\n",
    "        \n",
    "        if existing_tables:\n",
    "            print(f\"Found {len(existing_tables)} existing tables: {[table[0] for table in existing_tables]}\")\n",
    "            # Disable foreign key constraints temporarily to allow dropping tables\n",
    "            cursor.execute(\"PRAGMA foreign_keys = OFF;\")\n",
    "            \n",
    "            for table in existing_tables:\n",
    "                table_name = table[0]\n",
    "                cursor.execute(f\"DROP TABLE IF EXISTS {table_name};\")\n",
    "                print(f\"  Dropped table: {table_name}\")\n",
    "            \n",
    "            # Re-enable foreign key constraints\n",
    "            cursor.execute(\"PRAGMA foreign_keys = ON;\")\n",
    "            print(\"All existing tables dropped successfully.\")\n",
    "        else:\n",
    "            print(\"No existing tables found in database.\")\n",
    "\n",
    "        # --- Additional safety: Drop tables from schema.sql if any remain ---\n",
    "        tables_from_schema = tables_to_drop(schema_path)\n",
    "        for table in tables_from_schema:\n",
    "            cursor.execute(f\"DROP TABLE IF EXISTS {table};\")\n",
    "        print(\"Schema-based table cleanup completed.\")\n",
    "\n",
    "        # Read the content of the schema file using load_artifact.\n",
    "        schema_sql = load_artifact(schema_path)\n",
    "        \n",
    "        # Replace TIMESTAMP WITH TIME ZONE with TIMESTAMP for SQLite compatibility\n",
    "        if schema_sql:\n",
    "            schema_sql = schema_sql.replace(\"TIMESTAMP WITH TIME ZONE\", \"TIMESTAMP\")\n",
    "            cursor.executescript(schema_sql)\n",
    "            print(\"Tables created successfully.\")\n",
    "\n",
    "        # Check if the seed data file exists. If it does, load and execute it.\n",
    "        if os.path.exists(seed_path):\n",
    "            seed_sql = load_artifact(seed_path)\n",
    "            if seed_sql:\n",
    "                cursor.executescript(seed_sql)\n",
    "                print(\"Seed data inserted successfully.\")\n",
    "\n",
    "        # Commit the changes to the database.\n",
    "        conn.commit()\n",
    "        print(\"Database changes committed.\")    \n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Database error: {e}\")\n",
    "    finally:\n",
    "        # Ensure the connection is closed if it was opened.\n",
    "        if conn:\n",
    "            conn.close()\n",
    "            print(\"Database connection closed.\")\n",
    "\n",
    "def query_table(db_path, query, params=None):\n",
    "    if not os.path.exists(db_path):\n",
    "        print(f\"Error: Database file not found at {db_path}\")\n",
    "        return None\n",
    "    conn = None\n",
    "    try:\n",
    "        # Connect to the SQLite database\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        cursor = conn.cursor()\n",
    "        # Execute the query with or without parameters\n",
    "        if params:\n",
    "            cursor.execute(query, params)\n",
    "        else:\n",
    "            cursor.execute(query)\n",
    "        # Fetch all results\n",
    "        results = cursor.fetchall()\n",
    "        # Get column names for better readability (optional)\n",
    "        column_names = [description[0] for description in cursor.description]\n",
    "        print(f\"Query executed successfully. Found {len(results)} rows.\")\n",
    "        print(f\"Columns: {', '.join(column_names)}\")\n",
    "        return results\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Database query error: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        # Ensure the connection is closed if it was opened\n",
    "        if conn:\n",
    "            conn.close()\n",
    "try:\n",
    "    project_root\n",
    "except NameError:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "\n",
    "db_file = os.path.join(project_root, \"artifacts\", \"waffle_tech_suite.db\")\n",
    "schema_file = os.path.join(project_root, \"artifacts\", \"waffle_schema.sql\")\n",
    "seed_file = os.path.join(project_root, \"artifacts\", \"waffle_seed_data.sql\")\n",
    "\n",
    "# Execute the function\n",
    "create_database(db_file, schema_file, seed_file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the database was created and populated correctly\n",
    "print(\"=== Database Verification ===\")\n",
    "\n",
    "# Query user_roles table\n",
    "print(\"\\n--- User Roles ---\")\n",
    "roles_results = query_table(db_file, \"SELECT * FROM user_roles ORDER BY role_id\")\n",
    "if roles_results:\n",
    "    for row in roles_results:\n",
    "        print(f\"Role ID: {row[0]}, Role Name: {row[1]}\")\n",
    "\n",
    "# Query users table\n",
    "print(\"\\n--- Users ---\")\n",
    "users_results = query_table(db_file, \"SELECT * FROM users ORDER BY user_id\")\n",
    "if users_results:\n",
    "    for row in users_results:\n",
    "        print(f\"User ID: {row[0]}, Name: {row[1]} {row[2]}, Email: {row[3]}, Role ID: {row[5]}\")\n",
    "\n",
    "# Query with JOIN to show user roles\n",
    "print(\"\\n--- Users with Role Names ---\")\n",
    "join_results = query_table(db_file, \"\"\"\n",
    "    SELECT u.user_id, u.first_name, u.last_name, u.email, ur.role_name \n",
    "    FROM users u \n",
    "    JOIN user_roles ur ON u.role_id = ur.role_id \n",
    "    ORDER BY u.user_id\n",
    "\"\"\")\n",
    "if join_results:\n",
    "    for row in join_results:\n",
    "        print(f\"User: {row[1]} {row[2]} ({row[3]}) - Role: {row[4]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final status check\n",
    "import os\n",
    "\n",
    "print(\"=== Lab Completion Status ===\")\n",
    "print(f\"✅ Database file created: {os.path.exists(db_file)}\")\n",
    "print(f\"✅ Schema file exists: {os.path.exists(schema_file)}\")\n",
    "print(f\"✅ Seed data file exists: {os.path.exists(seed_file)}\")\n",
    "print(f\"✅ Database contains {len(users_results)} users\")\n",
    "print(f\"✅ Database contains {len(roles_results)} user roles\")\n",
    "print(f\"\\n🎉 Lab completed successfully!\")\n",
    "print(f\"Database location: {db_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write a prompt to generate the SQL schema from the PRD.\n",
    "additional_table_prompt = f\"\"\"\n",
    "You are a Senior Database Architect. Design a comprehensive, production-ready SQL database schema.\n",
    "I would like to add a new table to the existing schema {cleaned_schema} for displaying pre-defined onboarding tasks per user roles and their completion timeline. \n",
    "\n",
    "\n",
    "**Requirements:**\n",
    "- Create 1 table only (e.g., tasks_per_role)\n",
    "- This should be related to user_roles table via foreign key\n",
    "- Follow database normalization best practices \n",
    "- Use standard SQL naming conventions (lowercase with underscores)\n",
    "- Consider data integrity, scalability, and query performance\n",
    "- Use AUTO_INCREMENT for primary keys; instead\n",
    "- Use SQL Lite compatible syntax only\n",
    "\n",
    "**Output Format:**\n",
    "Provide ONLY the raw SQL CREATE TABLE statement for the new table (CREATE 1 TABLE ONLY) without:\n",
    "- Markdown code blocks or formatting\n",
    "- Explanatory text or comments\n",
    "- Additional documentation\n",
    "\n",
    " CREATE TABLE statement should be complete and executable. Remove any duplicate table definitions if any.\n",
    "**Existing schema:**\n",
    "{cleaned_schema}\n",
    "\"\"\"\n",
    "\n",
    "#enhanced_schema_prompt = prompt_enhancer(schema_prompt)\n",
    "print(\"--- Generating SQL Schema ---\")\n",
    "if prd_content:\n",
    "    additional_table = get_completion(additional_table_prompt, client, model_name, api_provider)\n",
    "    print(additional_table)\n",
    "\n",
    "    # Clean up the generated schema using our helper function\n",
    "    cleaned_table_schema = clean_llm_output(additional_table, language='sql')\n",
    "    print(cleaned_table_schema)\n",
    "\n",
    "    # Save the cleaned schema\n",
    "    save_artifact(cleaned_table_schema, 'artifacts/waffle_final_schema.sql',overwrite=True)\n",
    "else:\n",
    "    print(\"Skipping schema generation because PRD is missing.\")\n",
    "    cleaned_table_schema = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Conclusion\n",
    "\n",
    "Excellent work! You have now moved from abstract requirements to a concrete, physical database artifact. You've used an LLM to design a schema, generate realistic test data, and then used a Python script to bring that database to life. This `onboarding.db` file is the foundation upon which we will build our API in Day 3.\n",
    "\n",
    "> **Key Takeaway:** The ability to generate structured data definitions (like a SQL schema) from unstructured text (like a PRD) is a core skill in AI-assisted development. It automates a critical and often time-consuming design step."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
